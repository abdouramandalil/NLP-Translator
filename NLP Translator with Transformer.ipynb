{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m9JJ7FBw84tG"
   },
   "source": [
    "# Stage 1: Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZbcvtPlp3YWu"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.5.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQN8jwx48_yU"
   },
   "source": [
    "# Stage 2: Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPlOT-2mlw0r"
   },
   "source": [
    "## Loading files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCD9jwXsLwS_"
   },
   "source": [
    "We import files from our personal google drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"europarl-v7.fr-en.en\",mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    europarl_en=f.read()\n",
    "with open(\"europarl-v7.fr-en.fr\",mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    europarl_fr=f.read()\n",
    "with open(\"P85-Non-Breaking-Prefix.en\",mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    non_breaking_prefix_en=f.read()\n",
    "with open(\"P85-Non-Breaking-Prefix.fr\",mode=\"r\",encoding=\"utf-8\") as f:\n",
    "    non_breaking_prefix_fr=f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TEFw0D2vP_Dl"
   },
   "source": [
    "## Cleaning data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwIBeGXn7LIJ"
   },
   "source": [
    "Getting the non_breaking_prefixes as a clean list of words with a point at the end so it is easier to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "L_TeuktU40Cb"
   },
   "outputs": [],
   "source": [
    "non_breaking_prefix_en = non_breaking_prefix_en.split(\"\\n\")\n",
    "non_breaking_prefix_en = [' ' + pref + '.' for pref in non_breaking_prefix_en]\n",
    "non_breaking_prefix_fr = non_breaking_prefix_fr.split(\"\\n\")\n",
    "non_breaking_prefix_fr = [' ' + pref + '.' for pref in non_breaking_prefix_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9x4mZfKMaxD"
   },
   "source": [
    "We will need each word and other symbol that we want to keep to be in lower case and separated by spaces so we can \"tokenize\" them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Qg-8LLK-WdFp"
   },
   "outputs": [],
   "source": [
    "corpus_en = europarl_en\n",
    "# Add $$$ after non ending sentence points\n",
    "for prefix in non_breaking_prefix_en:\n",
    "    corpus_en = corpus_en.replace(prefix, prefix + '$$$')\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_en)\n",
    "# Remove $$$ markers\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "# Clear multiple spaces\n",
    "corpus_en = re.sub(r\"  +\", \" \", corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "corpus_fr = europarl_fr\n",
    "for prefix in non_breaking_prefix_fr:\n",
    "    corpus_fr = corpus_fr.replace(prefix, prefix + '$$$')\n",
    "corpus_fr = re.sub(r\"\\.(?=[0-9]|[a-z]|[A-Z])\", \".$$$\", corpus_fr)\n",
    "corpus_fr = re.sub(r\".\\$\\$\\$\", '', corpus_fr)\n",
    "corpus_fr = re.sub(r\"  +\", \" \", corpus_fr)\n",
    "corpus_fr = corpus_fr.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-Y9v8-Tozl2"
   },
   "source": [
    "## Tokenizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "p5YXanmOd_xK"
   },
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_en, target_vocab_size=2**13)\n",
    "tokenizer_fr = tfds.features.text.SubwordTextEncoder.build_from_corpus(\n",
    "    corpus_fr, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ftIbPzIwCtwL"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE_EN = tokenizer_en.vocab_size + 2 # = 8190\n",
    "VOCAB_SIZE_FR = tokenizer_fr.vocab_size + 2 # = 8171"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "oPFe2YJDC9jw"
   },
   "outputs": [],
   "source": [
    "inputs = [[VOCAB_SIZE_EN-2] + tokenizer_en.encode(sentence) + [VOCAB_SIZE_EN-1]\n",
    "          for sentence in corpus_en]\n",
    "outputs = [[VOCAB_SIZE_FR-2] + tokenizer_fr.encode(sentence) + [VOCAB_SIZE_FR-1]\n",
    "           for sentence in corpus_fr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bG6AlcFMpC5C"
   },
   "source": [
    "## Remove too long sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "F6CD6PLGyQWy"
   },
   "outputs": [],
   "source": [
    "MAX_LENGTH = 20\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs)\n",
    "                 if len(sent) > MAX_LENGTH]\n",
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ypm8h5aZQTZ1"
   },
   "source": [
    "## Inputs/outputs creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9FP0WPsdM8hl"
   },
   "source": [
    "As we train with batches, we need each input to have the same length. We pad with the appropriate token, and we will make sure this padding token doesn't interfere with our training later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "nvDfLDWUONlE"
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                       value=0,\n",
    "                                                       padding='post',\n",
    "                                                       maxlen=MAX_LENGTH)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs,\n",
    "                                                        value=0,\n",
    "                                                        padding='post',\n",
    "                                                        maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wFxMp3TOIYff"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ycT0YqydRcUd"
   },
   "source": [
    "# Stage 3: Model building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SBoH8G4XyR9"
   },
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7G9C3ucmJ86I"
   },
   "source": [
    "Positional encoding formulae:\n",
    "\n",
    "$PE_{(pos,2i)} =\\sin(pos/10000^{2i/dmodel})$\n",
    "\n",
    "$PE_{(pos,2i+1)} =\\cos(pos/10000^{2i/dmodel})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "e2wc6sYlX0dr"
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i//2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "\n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis],\n",
    "                                 np.arange(d_model)[np.newaxis, :],\n",
    "                                 d_model)\n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcw8YIQqRhOJ"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3sffhwwvX-wj"
   },
   "source": [
    "### Attention computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7VBuW6lESLDX"
   },
   "source": [
    "$Attention(Q, K, V ) = \\text{softmax}\\left(\\dfrac{QK^T}{\\sqrt{d_k}}\\right)V $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "2rEoCNJURbrT"
   },
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    \n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9)\n",
    "    \n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    \n",
    "    return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MjtvXrfYEx7"
   },
   "source": [
    "### Multi-head attention sublayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "lvq4I9uTX5p7"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "        \n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "        \n",
    "        self.query_lin = layers.Dense(units=self.d_model)\n",
    "        self.key_lin = layers.Dense(units=self.d_model)\n",
    "        self.value_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "        self.final_lin = layers.Dense(units=self.d_model)\n",
    "        \n",
    "    def split_proj(self, inputs, batch_size): # inputs: (batch_size, seq_length, d_model)\n",
    "        shape = (batch_size,\n",
    "                 -1,\n",
    "                 self.nb_proj,\n",
    "                 self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape=shape) # (batch_size, seq_length, nb_proj, d_proj)\n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) # (batch_size, nb_proj, seq_length, d_proj)\n",
    "    \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "        \n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "        \n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "        \n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "        \n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "        \n",
    "        concat_attention = tf.reshape(attention,\n",
    "                                      shape=(batch_size, -1, self.d_model))\n",
    "        \n",
    "        outputs = self.final_lin(concat_attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yiyuHe1OeT5N"
   },
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UV0ZMH7KT_KZ"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs,\n",
    "                                              inputs,\n",
    "                                              inputs,\n",
    "                                              mask)\n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "P-P92KeZih60"
   },
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"encoder\"):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DthraBEwuvl"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "7ZWZyFBnwy8u"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        # Self multi head attention\n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Multi head attention combined with encoder output\n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        # Feed foward\n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units,\n",
    "                                    activation=\"relu\")\n",
    "        self.dense_2 = layers.Dense(units=self.d_model)\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs,\n",
    "                                                inputs,\n",
    "                                                inputs,\n",
    "                                                mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention,\n",
    "                                                  enc_outputs,\n",
    "                                                  enc_outputs,\n",
    "                                                  mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "kpzdiWHiwywF"
   },
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 vocab_size,\n",
    "                 d_model,\n",
    "                 name=\"decoder\"):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units,\n",
    "                                        nb_proj,\n",
    "                                        dropout_rate) \n",
    "                           for i in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs,\n",
    "                                         enc_outputs,\n",
    "                                         mask_1,\n",
    "                                         mask_2,\n",
    "                                         training)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x5sJYkjbz5DD"
   },
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "GqvqNjJPwyh-"
   },
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocab_size_enc,\n",
    "                 vocab_size_dec,\n",
    "                 d_model,\n",
    "                 nb_layers,\n",
    "                 FFN_units,\n",
    "                 nb_proj,\n",
    "                 dropout_rate,\n",
    "                 name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "        \n",
    "        self.encoder = Encoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_enc,\n",
    "                               d_model)\n",
    "        self.decoder = Decoder(nb_layers,\n",
    "                               FFN_units,\n",
    "                               nb_proj,\n",
    "                               dropout_rate,\n",
    "                               vocab_size_dec,\n",
    "                               d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name=\"lin_ouput\")\n",
    "    \n",
    "    def create_padding_mask(self, seq):\n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahead_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahead_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(\n",
    "            self.create_padding_mask(dec_inputs),\n",
    "            self.create_look_ahead_mask(dec_inputs)\n",
    "        )\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "        \n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs,\n",
    "                                   enc_outputs,\n",
    "                                   dec_mask_1,\n",
    "                                   dec_mask_2,\n",
    "                                   training)\n",
    "        \n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-c-LRThUPrso"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "qiOdqQ5qPs8z"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Hyper-parameters\n",
    "D_MODEL = 128 # 512\n",
    "NB_LAYERS = 4 # 6\n",
    "FFN_UNITS = 512 # 2048\n",
    "NB_PROJ = 8 # 8\n",
    "DROPOUT_RATE = 0.1 # 0.1\n",
    "\n",
    "transformer = Transformer(vocab_size_enc=VOCAB_SIZE_EN,\n",
    "                          vocab_size_dec=VOCAB_SIZE_FR,\n",
    "                          d_model=D_MODEL,\n",
    "                          nb_layers=NB_LAYERS,\n",
    "                          FFN_units=FFN_UNITS,\n",
    "                          nb_proj=NB_PROJ,\n",
    "                          dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "46xg4Wrg1Wgl"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                                            reduction=\"none\")\n",
    "\n",
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss_)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name=\"train_loss\")\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name=\"train_accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4Goque362343"
   },
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps**-1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "leaning_rate = CustomSchedule(D_MODEL)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(leaning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "Nb_32PIU5Zkh",
    "outputId": "f3ea9cb2-bf36-4126-ade2-266dc6029528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "checkpoint_path = \"./TF/ckpt/\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print(\"Latest checkpoint restored!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lhFK5kUx602K"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start of epoch 1\n",
      "Epoch 1 Batch 0 Loss 1.7042 Accuracy 0.4054\n",
      "Epoch 1 Batch 50 Loss 1.7087 Accuracy 0.3829\n",
      "Epoch 1 Batch 100 Loss 1.7013 Accuracy 0.3857\n",
      "Epoch 1 Batch 150 Loss 1.6940 Accuracy 0.3868\n",
      "Epoch 1 Batch 200 Loss 1.6922 Accuracy 0.3863\n",
      "Epoch 1 Batch 250 Loss 1.6849 Accuracy 0.3861\n",
      "Epoch 1 Batch 300 Loss 1.6783 Accuracy 0.3869\n",
      "Epoch 1 Batch 350 Loss 1.6688 Accuracy 0.3871\n",
      "Epoch 1 Batch 400 Loss 1.6663 Accuracy 0.3876\n",
      "Epoch 1 Batch 450 Loss 1.6634 Accuracy 0.3881\n",
      "Epoch 1 Batch 500 Loss 1.6598 Accuracy 0.3882\n",
      "Epoch 1 Batch 550 Loss 1.6506 Accuracy 0.3890\n",
      "Epoch 1 Batch 600 Loss 1.6499 Accuracy 0.3893\n",
      "Epoch 1 Batch 650 Loss 1.6457 Accuracy 0.3899\n",
      "Epoch 1 Batch 700 Loss 1.6370 Accuracy 0.3911\n",
      "Epoch 1 Batch 750 Loss 1.6340 Accuracy 0.3918\n",
      "Epoch 1 Batch 800 Loss 1.6316 Accuracy 0.3924\n",
      "Epoch 1 Batch 850 Loss 1.6293 Accuracy 0.3929\n",
      "Epoch 1 Batch 900 Loss 1.6264 Accuracy 0.3935\n",
      "Epoch 1 Batch 950 Loss 1.6218 Accuracy 0.3937\n",
      "Epoch 1 Batch 1000 Loss 1.6175 Accuracy 0.3939\n",
      "Epoch 1 Batch 1050 Loss 1.6158 Accuracy 0.3943\n",
      "Epoch 1 Batch 1100 Loss 1.6124 Accuracy 0.3946\n",
      "Epoch 1 Batch 1150 Loss 1.6084 Accuracy 0.3952\n",
      "Epoch 1 Batch 1200 Loss 1.6058 Accuracy 0.3956\n",
      "Epoch 1 Batch 1250 Loss 1.6031 Accuracy 0.3961\n",
      "Epoch 1 Batch 1300 Loss 1.6000 Accuracy 0.3966\n",
      "Epoch 1 Batch 1350 Loss 1.5966 Accuracy 0.3973\n",
      "Epoch 1 Batch 1400 Loss 1.5932 Accuracy 0.3980\n",
      "Epoch 1 Batch 1450 Loss 1.5892 Accuracy 0.3987\n",
      "Epoch 1 Batch 1500 Loss 1.5851 Accuracy 0.3998\n",
      "Epoch 1 Batch 1550 Loss 1.5802 Accuracy 0.4007\n",
      "Epoch 1 Batch 1600 Loss 1.5759 Accuracy 0.4016\n",
      "Epoch 1 Batch 1650 Loss 1.5715 Accuracy 0.4025\n",
      "Epoch 1 Batch 1700 Loss 1.5681 Accuracy 0.4035\n",
      "Epoch 1 Batch 1750 Loss 1.5645 Accuracy 0.4044\n",
      "Epoch 1 Batch 1800 Loss 1.5611 Accuracy 0.4054\n",
      "Epoch 1 Batch 1850 Loss 1.5571 Accuracy 0.4063\n",
      "Epoch 1 Batch 1900 Loss 1.5530 Accuracy 0.4072\n",
      "Epoch 1 Batch 1950 Loss 1.5494 Accuracy 0.4081\n",
      "Epoch 1 Batch 2000 Loss 1.5454 Accuracy 0.4089\n",
      "Epoch 1 Batch 2050 Loss 1.5417 Accuracy 0.4097\n",
      "Epoch 1 Batch 2100 Loss 1.5376 Accuracy 0.4103\n",
      "Epoch 1 Batch 2150 Loss 1.5332 Accuracy 0.4108\n",
      "Epoch 1 Batch 2200 Loss 1.5285 Accuracy 0.4113\n",
      "Epoch 1 Batch 2250 Loss 1.5232 Accuracy 0.4118\n",
      "Epoch 1 Batch 2300 Loss 1.5182 Accuracy 0.4122\n",
      "Epoch 1 Batch 2350 Loss 1.5138 Accuracy 0.4128\n",
      "Epoch 1 Batch 2400 Loss 1.5091 Accuracy 0.4131\n",
      "Epoch 1 Batch 2450 Loss 1.5034 Accuracy 0.4137\n",
      "Epoch 1 Batch 2500 Loss 1.4987 Accuracy 0.4142\n",
      "Epoch 1 Batch 2550 Loss 1.4944 Accuracy 0.4147\n",
      "Epoch 1 Batch 2600 Loss 1.4906 Accuracy 0.4153\n",
      "Epoch 1 Batch 2650 Loss 1.4856 Accuracy 0.4160\n",
      "Epoch 1 Batch 2700 Loss 1.4813 Accuracy 0.4166\n",
      "Epoch 1 Batch 2750 Loss 1.4775 Accuracy 0.4171\n",
      "Epoch 1 Batch 2800 Loss 1.4733 Accuracy 0.4177\n",
      "Epoch 1 Batch 2850 Loss 1.4695 Accuracy 0.4182\n",
      "Epoch 1 Batch 2900 Loss 1.4658 Accuracy 0.4187\n",
      "Epoch 1 Batch 2950 Loss 1.4617 Accuracy 0.4193\n",
      "Epoch 1 Batch 3000 Loss 1.4575 Accuracy 0.4199\n",
      "Epoch 1 Batch 3050 Loss 1.4536 Accuracy 0.4205\n",
      "Epoch 1 Batch 3100 Loss 1.4501 Accuracy 0.4210\n",
      "Epoch 1 Batch 3150 Loss 1.4467 Accuracy 0.4215\n",
      "Epoch 1 Batch 3200 Loss 1.4430 Accuracy 0.4219\n",
      "Epoch 1 Batch 3250 Loss 1.4387 Accuracy 0.4224\n",
      "Epoch 1 Batch 3300 Loss 1.4349 Accuracy 0.4229\n",
      "Epoch 1 Batch 3350 Loss 1.4310 Accuracy 0.4235\n",
      "Epoch 1 Batch 3400 Loss 1.4274 Accuracy 0.4240\n",
      "Epoch 1 Batch 3450 Loss 1.4238 Accuracy 0.4245\n",
      "Epoch 1 Batch 3500 Loss 1.4208 Accuracy 0.4251\n",
      "Epoch 1 Batch 3550 Loss 1.4175 Accuracy 0.4257\n",
      "Epoch 1 Batch 3600 Loss 1.4144 Accuracy 0.4263\n",
      "Epoch 1 Batch 3650 Loss 1.4110 Accuracy 0.4268\n",
      "Epoch 1 Batch 3700 Loss 1.4075 Accuracy 0.4274\n",
      "Epoch 1 Batch 3750 Loss 1.4041 Accuracy 0.4279\n",
      "Epoch 1 Batch 3800 Loss 1.4009 Accuracy 0.4284\n",
      "Epoch 1 Batch 3850 Loss 1.3980 Accuracy 0.4289\n",
      "Epoch 1 Batch 3900 Loss 1.3946 Accuracy 0.4294\n",
      "Epoch 1 Batch 3950 Loss 1.3914 Accuracy 0.4300\n",
      "Epoch 1 Batch 4000 Loss 1.3883 Accuracy 0.4306\n",
      "Epoch 1 Batch 4050 Loss 1.3853 Accuracy 0.4312\n",
      "Epoch 1 Batch 4100 Loss 1.3827 Accuracy 0.4316\n",
      "Epoch 1 Batch 4150 Loss 1.3808 Accuracy 0.4319\n",
      "Epoch 1 Batch 4200 Loss 1.3799 Accuracy 0.4321\n",
      "Epoch 1 Batch 4250 Loss 1.3789 Accuracy 0.4323\n",
      "Epoch 1 Batch 4300 Loss 1.3787 Accuracy 0.4324\n",
      "Epoch 1 Batch 4350 Loss 1.3784 Accuracy 0.4325\n",
      "Epoch 1 Batch 4400 Loss 1.3781 Accuracy 0.4326\n",
      "Epoch 1 Batch 4450 Loss 1.3779 Accuracy 0.4326\n",
      "Epoch 1 Batch 4500 Loss 1.3780 Accuracy 0.4327\n",
      "Epoch 1 Batch 4550 Loss 1.3777 Accuracy 0.4327\n",
      "Epoch 1 Batch 4600 Loss 1.3776 Accuracy 0.4327\n",
      "Epoch 1 Batch 4650 Loss 1.3776 Accuracy 0.4327\n",
      "Epoch 1 Batch 4700 Loss 1.3779 Accuracy 0.4327\n",
      "Epoch 1 Batch 4750 Loss 1.3778 Accuracy 0.4328\n",
      "Epoch 1 Batch 4800 Loss 1.3775 Accuracy 0.4328\n",
      "Epoch 1 Batch 4850 Loss 1.3772 Accuracy 0.4329\n",
      "Epoch 1 Batch 4900 Loss 1.3770 Accuracy 0.4330\n",
      "Epoch 1 Batch 4950 Loss 1.3770 Accuracy 0.4330\n",
      "Epoch 1 Batch 5000 Loss 1.3769 Accuracy 0.4330\n",
      "Epoch 1 Batch 5050 Loss 1.3771 Accuracy 0.4329\n",
      "Epoch 1 Batch 5100 Loss 1.3770 Accuracy 0.4329\n",
      "Epoch 1 Batch 5150 Loss 1.3770 Accuracy 0.4328\n",
      "Epoch 1 Batch 5200 Loss 1.3770 Accuracy 0.4327\n",
      "Epoch 1 Batch 5250 Loss 1.3769 Accuracy 0.4327\n",
      "Epoch 1 Batch 5300 Loss 1.3766 Accuracy 0.4326\n",
      "Epoch 1 Batch 5350 Loss 1.3764 Accuracy 0.4325\n",
      "Epoch 1 Batch 5400 Loss 1.3761 Accuracy 0.4325\n",
      "Epoch 1 Batch 5450 Loss 1.3760 Accuracy 0.4324\n",
      "Epoch 1 Batch 5500 Loss 1.3760 Accuracy 0.4323\n",
      "Epoch 1 Batch 5550 Loss 1.3756 Accuracy 0.4322\n",
      "Epoch 1 Batch 5600 Loss 1.3752 Accuracy 0.4322\n",
      "Epoch 1 Batch 5650 Loss 1.3746 Accuracy 0.4322\n",
      "Epoch 1 Batch 5700 Loss 1.3745 Accuracy 0.4322\n",
      "Saving checkpoint for epoch 1 at ./TF/ckpt/ckpt-2\n",
      "Time taken for 1 epoch: 1472.5271487236023 secs\n",
      "\n",
      "Start of epoch 2\n",
      "Epoch 2 Batch 0 Loss 1.4293 Accuracy 0.4161\n",
      "Epoch 2 Batch 50 Loss 1.3289 Accuracy 0.4333\n",
      "Epoch 2 Batch 100 Loss 1.3273 Accuracy 0.4346\n",
      "Epoch 2 Batch 150 Loss 1.3230 Accuracy 0.4371\n",
      "Epoch 2 Batch 200 Loss 1.3250 Accuracy 0.4370\n",
      "Epoch 2 Batch 250 Loss 1.3270 Accuracy 0.4369\n",
      "Epoch 2 Batch 300 Loss 1.3264 Accuracy 0.4374\n",
      "Epoch 2 Batch 350 Loss 1.3264 Accuracy 0.4375\n",
      "Epoch 2 Batch 400 Loss 1.3234 Accuracy 0.4375\n",
      "Epoch 2 Batch 450 Loss 1.3235 Accuracy 0.4375\n",
      "Epoch 2 Batch 500 Loss 1.3218 Accuracy 0.4375\n",
      "Epoch 2 Batch 550 Loss 1.3187 Accuracy 0.4374\n",
      "Epoch 2 Batch 600 Loss 1.3166 Accuracy 0.4374\n",
      "Epoch 2 Batch 650 Loss 1.3177 Accuracy 0.4373\n",
      "Epoch 2 Batch 700 Loss 1.3170 Accuracy 0.4383\n",
      "Epoch 2 Batch 750 Loss 1.3155 Accuracy 0.4386\n",
      "Epoch 2 Batch 800 Loss 1.3134 Accuracy 0.4390\n",
      "Epoch 2 Batch 850 Loss 1.3121 Accuracy 0.4395\n",
      "Epoch 2 Batch 900 Loss 1.3105 Accuracy 0.4394\n",
      "Epoch 2 Batch 950 Loss 1.3095 Accuracy 0.4394\n",
      "Epoch 2 Batch 1000 Loss 1.3067 Accuracy 0.4397\n",
      "Epoch 2 Batch 1050 Loss 1.3050 Accuracy 0.4399\n",
      "Epoch 2 Batch 1100 Loss 1.3038 Accuracy 0.4400\n",
      "Epoch 2 Batch 1150 Loss 1.3030 Accuracy 0.4403\n",
      "Epoch 2 Batch 1200 Loss 1.3002 Accuracy 0.4405\n",
      "Epoch 2 Batch 1250 Loss 1.2971 Accuracy 0.4411\n",
      "Epoch 2 Batch 1300 Loss 1.2949 Accuracy 0.4416\n",
      "Epoch 2 Batch 1350 Loss 1.2908 Accuracy 0.4422\n",
      "Epoch 2 Batch 1400 Loss 1.2885 Accuracy 0.4428\n",
      "Epoch 2 Batch 1450 Loss 1.2859 Accuracy 0.4435\n",
      "Epoch 2 Batch 1500 Loss 1.2834 Accuracy 0.4444\n",
      "Epoch 2 Batch 1550 Loss 1.2800 Accuracy 0.4452\n",
      "Epoch 2 Batch 1600 Loss 1.2776 Accuracy 0.4461\n",
      "Epoch 2 Batch 1650 Loss 1.2754 Accuracy 0.4468\n",
      "Epoch 2 Batch 1700 Loss 1.2729 Accuracy 0.4476\n",
      "Epoch 2 Batch 1750 Loss 1.2704 Accuracy 0.4485\n",
      "Epoch 2 Batch 1800 Loss 1.2673 Accuracy 0.4493\n",
      "Epoch 2 Batch 1850 Loss 1.2652 Accuracy 0.4502\n",
      "Epoch 2 Batch 1900 Loss 1.2627 Accuracy 0.4509\n",
      "Epoch 2 Batch 1950 Loss 1.2606 Accuracy 0.4517\n",
      "Epoch 2 Batch 2000 Loss 1.2577 Accuracy 0.4524\n",
      "Epoch 2 Batch 2050 Loss 1.2553 Accuracy 0.4528\n",
      "Epoch 2 Batch 2100 Loss 1.2523 Accuracy 0.4532\n",
      "Epoch 2 Batch 2150 Loss 1.2492 Accuracy 0.4535\n",
      "Epoch 2 Batch 2200 Loss 1.2454 Accuracy 0.4538\n",
      "Epoch 2 Batch 2250 Loss 1.2417 Accuracy 0.4540\n",
      "Epoch 2 Batch 2300 Loss 1.2383 Accuracy 0.4542\n",
      "Epoch 2 Batch 2350 Loss 1.2351 Accuracy 0.4545\n",
      "Epoch 2 Batch 2400 Loss 1.2316 Accuracy 0.4547\n",
      "Epoch 2 Batch 2450 Loss 1.2286 Accuracy 0.4551\n",
      "Epoch 2 Batch 2500 Loss 1.2248 Accuracy 0.4555\n",
      "Epoch 2 Batch 2550 Loss 1.2218 Accuracy 0.4559\n",
      "Epoch 2 Batch 2600 Loss 1.2188 Accuracy 0.4563\n",
      "Epoch 2 Batch 2650 Loss 1.2156 Accuracy 0.4568\n",
      "Epoch 2 Batch 2700 Loss 1.2123 Accuracy 0.4572\n",
      "Epoch 2 Batch 2750 Loss 1.2092 Accuracy 0.4576\n",
      "Epoch 2 Batch 2800 Loss 1.2061 Accuracy 0.4580\n",
      "Epoch 2 Batch 2850 Loss 1.2035 Accuracy 0.4584\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Batch 2900 Loss 1.2013 Accuracy 0.4587\n",
      "Epoch 2 Batch 2950 Loss 1.1985 Accuracy 0.4590\n",
      "Epoch 2 Batch 3000 Loss 1.1960 Accuracy 0.4594\n",
      "Epoch 2 Batch 3050 Loss 1.1941 Accuracy 0.4597\n",
      "Epoch 2 Batch 3100 Loss 1.1916 Accuracy 0.4601\n",
      "Epoch 2 Batch 3150 Loss 1.1889 Accuracy 0.4604\n",
      "Epoch 2 Batch 3200 Loss 1.1865 Accuracy 0.4607\n",
      "Epoch 2 Batch 3250 Loss 1.1842 Accuracy 0.4611\n",
      "Epoch 2 Batch 3300 Loss 1.1814 Accuracy 0.4614\n",
      "Epoch 2 Batch 3350 Loss 1.1787 Accuracy 0.4617\n",
      "Epoch 2 Batch 3400 Loss 1.1764 Accuracy 0.4620\n",
      "Epoch 2 Batch 3450 Loss 1.1739 Accuracy 0.4624\n",
      "Epoch 2 Batch 3500 Loss 1.1722 Accuracy 0.4628\n",
      "Epoch 2 Batch 3550 Loss 1.1698 Accuracy 0.4632\n",
      "Epoch 2 Batch 3600 Loss 1.1673 Accuracy 0.4636\n",
      "Epoch 2 Batch 3650 Loss 1.1648 Accuracy 0.4639\n",
      "Epoch 2 Batch 3700 Loss 1.1626 Accuracy 0.4644\n",
      "Epoch 2 Batch 3750 Loss 1.1606 Accuracy 0.4648\n",
      "Epoch 2 Batch 3800 Loss 1.1586 Accuracy 0.4651\n",
      "Epoch 2 Batch 3850 Loss 1.1567 Accuracy 0.4656\n",
      "Epoch 2 Batch 3900 Loss 1.1548 Accuracy 0.4660\n",
      "Epoch 2 Batch 3950 Loss 1.1533 Accuracy 0.4664\n",
      "Epoch 2 Batch 4000 Loss 1.1517 Accuracy 0.4668\n",
      "Epoch 2 Batch 4050 Loss 1.1499 Accuracy 0.4672\n",
      "Epoch 2 Batch 4100 Loss 1.1484 Accuracy 0.4675\n",
      "Epoch 2 Batch 4150 Loss 1.1473 Accuracy 0.4677\n",
      "Epoch 2 Batch 4200 Loss 1.1468 Accuracy 0.4678\n",
      "Epoch 2 Batch 4250 Loss 1.1469 Accuracy 0.4678\n",
      "Epoch 2 Batch 4300 Loss 1.1476 Accuracy 0.4678\n",
      "Epoch 2 Batch 4350 Loss 1.1482 Accuracy 0.4678\n",
      "Epoch 2 Batch 4400 Loss 1.1487 Accuracy 0.4677\n",
      "Epoch 2 Batch 4450 Loss 1.1496 Accuracy 0.4676\n",
      "Epoch 2 Batch 4500 Loss 1.1501 Accuracy 0.4675\n",
      "Epoch 2 Batch 4550 Loss 1.1510 Accuracy 0.4673\n",
      "Epoch 2 Batch 4600 Loss 1.1522 Accuracy 0.4672\n",
      "Epoch 2 Batch 4650 Loss 1.1531 Accuracy 0.4671\n",
      "Epoch 2 Batch 4700 Loss 1.1541 Accuracy 0.4670\n",
      "Epoch 2 Batch 4750 Loss 1.1550 Accuracy 0.4669\n",
      "Epoch 2 Batch 4800 Loss 1.1557 Accuracy 0.4668\n",
      "Epoch 2 Batch 4850 Loss 1.1568 Accuracy 0.4666\n",
      "Epoch 2 Batch 4900 Loss 1.1576 Accuracy 0.4665\n",
      "Epoch 2 Batch 4950 Loss 1.1582 Accuracy 0.4663\n",
      "Epoch 2 Batch 5000 Loss 1.1591 Accuracy 0.4662\n",
      "Epoch 2 Batch 5050 Loss 1.1599 Accuracy 0.4660\n",
      "Epoch 2 Batch 5100 Loss 1.1607 Accuracy 0.4658\n",
      "Epoch 2 Batch 5150 Loss 1.1616 Accuracy 0.4657\n",
      "Epoch 2 Batch 5200 Loss 1.1625 Accuracy 0.4655\n",
      "Epoch 2 Batch 5250 Loss 1.1632 Accuracy 0.4653\n",
      "Epoch 2 Batch 5300 Loss 1.1638 Accuracy 0.4651\n",
      "Epoch 2 Batch 5350 Loss 1.1645 Accuracy 0.4649\n",
      "Epoch 2 Batch 5400 Loss 1.1651 Accuracy 0.4647\n",
      "Epoch 2 Batch 5450 Loss 1.1657 Accuracy 0.4646\n",
      "Epoch 2 Batch 5500 Loss 1.1663 Accuracy 0.4644\n",
      "Epoch 2 Batch 5550 Loss 1.1668 Accuracy 0.4641\n",
      "Epoch 2 Batch 5600 Loss 1.1675 Accuracy 0.4639\n",
      "Epoch 2 Batch 5650 Loss 1.1680 Accuracy 0.4638\n",
      "Epoch 2 Batch 5700 Loss 1.1685 Accuracy 0.4637\n",
      "Saving checkpoint for epoch 2 at ./TF/ckpt/ckpt-3\n",
      "Time taken for 1 epoch: 1467.6673719882965 secs\n",
      "\n",
      "Start of epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.3665 Accuracy 0.4770\n",
      "Epoch 3 Batch 50 Loss 1.2514 Accuracy 0.4528\n",
      "Epoch 3 Batch 100 Loss 1.2338 Accuracy 0.4537\n",
      "Epoch 3 Batch 150 Loss 1.2321 Accuracy 0.4551\n",
      "Epoch 3 Batch 200 Loss 1.2272 Accuracy 0.4556\n",
      "Epoch 3 Batch 250 Loss 1.2219 Accuracy 0.4565\n",
      "Epoch 3 Batch 300 Loss 1.2187 Accuracy 0.4558\n",
      "Epoch 3 Batch 350 Loss 1.2150 Accuracy 0.4556\n",
      "Epoch 3 Batch 400 Loss 1.2115 Accuracy 0.4555\n",
      "Epoch 3 Batch 450 Loss 1.2075 Accuracy 0.4551\n",
      "Epoch 3 Batch 500 Loss 1.2037 Accuracy 0.4545\n",
      "Epoch 3 Batch 550 Loss 1.2036 Accuracy 0.4546\n",
      "Epoch 3 Batch 600 Loss 1.2037 Accuracy 0.4551\n",
      "Epoch 3 Batch 650 Loss 1.2016 Accuracy 0.4556\n",
      "Epoch 3 Batch 700 Loss 1.1998 Accuracy 0.4563\n",
      "Epoch 3 Batch 750 Loss 1.1989 Accuracy 0.4567\n",
      "Epoch 3 Batch 800 Loss 1.1978 Accuracy 0.4569\n",
      "Epoch 3 Batch 850 Loss 1.1965 Accuracy 0.4568\n",
      "Epoch 3 Batch 900 Loss 1.1952 Accuracy 0.4572\n",
      "Epoch 3 Batch 950 Loss 1.1947 Accuracy 0.4572\n",
      "Epoch 3 Batch 1000 Loss 1.1930 Accuracy 0.4571\n",
      "Epoch 3 Batch 1050 Loss 1.1924 Accuracy 0.4571\n",
      "Epoch 3 Batch 1100 Loss 1.1917 Accuracy 0.4571\n",
      "Epoch 3 Batch 1150 Loss 1.1908 Accuracy 0.4572\n",
      "Epoch 3 Batch 1200 Loss 1.1883 Accuracy 0.4575\n",
      "Epoch 3 Batch 1250 Loss 1.1867 Accuracy 0.4577\n",
      "Epoch 3 Batch 1300 Loss 1.1852 Accuracy 0.4581\n",
      "Epoch 3 Batch 1350 Loss 1.1825 Accuracy 0.4587\n",
      "Epoch 3 Batch 1400 Loss 1.1803 Accuracy 0.4594\n",
      "Epoch 3 Batch 1450 Loss 1.1776 Accuracy 0.4601\n",
      "Epoch 3 Batch 1500 Loss 1.1749 Accuracy 0.4611\n",
      "Epoch 3 Batch 1550 Loss 1.1723 Accuracy 0.4619\n",
      "Epoch 3 Batch 1600 Loss 1.1690 Accuracy 0.4627\n",
      "Epoch 3 Batch 1650 Loss 1.1671 Accuracy 0.4635\n",
      "Epoch 3 Batch 1700 Loss 1.1652 Accuracy 0.4641\n",
      "Epoch 3 Batch 1750 Loss 1.1629 Accuracy 0.4649\n",
      "Epoch 3 Batch 1800 Loss 1.1599 Accuracy 0.4658\n",
      "Epoch 3 Batch 1850 Loss 1.1581 Accuracy 0.4667\n",
      "Epoch 3 Batch 1900 Loss 1.1554 Accuracy 0.4675\n",
      "Epoch 3 Batch 1950 Loss 1.1535 Accuracy 0.4683\n",
      "Epoch 3 Batch 2000 Loss 1.1515 Accuracy 0.4688\n",
      "Epoch 3 Batch 2050 Loss 1.1495 Accuracy 0.4694\n",
      "Epoch 3 Batch 2100 Loss 1.1471 Accuracy 0.4697\n",
      "Epoch 3 Batch 2150 Loss 1.1447 Accuracy 0.4699\n",
      "Epoch 3 Batch 2200 Loss 1.1419 Accuracy 0.4701\n",
      "Epoch 3 Batch 2250 Loss 1.1383 Accuracy 0.4704\n",
      "Epoch 3 Batch 2300 Loss 1.1348 Accuracy 0.4707\n",
      "Epoch 3 Batch 2350 Loss 1.1315 Accuracy 0.4709\n",
      "Epoch 3 Batch 2400 Loss 1.1283 Accuracy 0.4711\n",
      "Epoch 3 Batch 2450 Loss 1.1248 Accuracy 0.4714\n",
      "Epoch 3 Batch 2500 Loss 1.1214 Accuracy 0.4717\n",
      "Epoch 3 Batch 2550 Loss 1.1184 Accuracy 0.4721\n",
      "Epoch 3 Batch 2600 Loss 1.1155 Accuracy 0.4725\n",
      "Epoch 3 Batch 2650 Loss 1.1127 Accuracy 0.4730\n",
      "Epoch 3 Batch 2700 Loss 1.1098 Accuracy 0.4734\n",
      "Epoch 3 Batch 2750 Loss 1.1074 Accuracy 0.4738\n",
      "Epoch 3 Batch 2800 Loss 1.1048 Accuracy 0.4740\n",
      "Epoch 3 Batch 2850 Loss 1.1025 Accuracy 0.4744\n",
      "Epoch 3 Batch 2900 Loss 1.0998 Accuracy 0.4747\n",
      "Epoch 3 Batch 2950 Loss 1.0976 Accuracy 0.4750\n",
      "Epoch 3 Batch 3000 Loss 1.0953 Accuracy 0.4752\n",
      "Epoch 3 Batch 3050 Loss 1.0935 Accuracy 0.4755\n",
      "Epoch 3 Batch 3100 Loss 1.0912 Accuracy 0.4758\n",
      "Epoch 3 Batch 3150 Loss 1.0892 Accuracy 0.4761\n",
      "Epoch 3 Batch 3200 Loss 1.0871 Accuracy 0.4764\n",
      "Epoch 3 Batch 3250 Loss 1.0851 Accuracy 0.4766\n",
      "Epoch 3 Batch 3300 Loss 1.0828 Accuracy 0.4769\n",
      "Epoch 3 Batch 3350 Loss 1.0807 Accuracy 0.4773\n",
      "Epoch 3 Batch 3400 Loss 1.0783 Accuracy 0.4776\n",
      "Epoch 3 Batch 3450 Loss 1.0765 Accuracy 0.4779\n",
      "Epoch 3 Batch 3500 Loss 1.0746 Accuracy 0.4783\n",
      "Epoch 3 Batch 3550 Loss 1.0724 Accuracy 0.4788\n",
      "Epoch 3 Batch 3600 Loss 1.0701 Accuracy 0.4791\n",
      "Epoch 3 Batch 3650 Loss 1.0684 Accuracy 0.4794\n",
      "Epoch 3 Batch 3700 Loss 1.0666 Accuracy 0.4798\n",
      "Epoch 3 Batch 3750 Loss 1.0648 Accuracy 0.4801\n",
      "Epoch 3 Batch 3800 Loss 1.0629 Accuracy 0.4805\n",
      "Epoch 3 Batch 3850 Loss 1.0612 Accuracy 0.4809\n",
      "Epoch 3 Batch 3900 Loss 1.0596 Accuracy 0.4813\n",
      "Epoch 3 Batch 3950 Loss 1.0580 Accuracy 0.4816\n",
      "Epoch 3 Batch 4000 Loss 1.0564 Accuracy 0.4820\n",
      "Epoch 3 Batch 4050 Loss 1.0548 Accuracy 0.4823\n",
      "Epoch 3 Batch 4100 Loss 1.0537 Accuracy 0.4826\n",
      "Epoch 3 Batch 4150 Loss 1.0532 Accuracy 0.4827\n",
      "Epoch 3 Batch 4200 Loss 1.0533 Accuracy 0.4827\n",
      "Epoch 3 Batch 4250 Loss 1.0532 Accuracy 0.4828\n",
      "Epoch 3 Batch 4300 Loss 1.0536 Accuracy 0.4827\n",
      "Epoch 3 Batch 4350 Loss 1.0546 Accuracy 0.4826\n",
      "Epoch 3 Batch 4400 Loss 1.0557 Accuracy 0.4825\n",
      "Epoch 3 Batch 4450 Loss 1.0568 Accuracy 0.4823\n",
      "Epoch 3 Batch 4500 Loss 1.0579 Accuracy 0.4821\n",
      "Epoch 3 Batch 4550 Loss 1.0590 Accuracy 0.4820\n",
      "Epoch 3 Batch 4600 Loss 1.0602 Accuracy 0.4818\n",
      "Epoch 3 Batch 4650 Loss 1.0614 Accuracy 0.4817\n",
      "Epoch 3 Batch 4700 Loss 1.0626 Accuracy 0.4815\n",
      "Epoch 3 Batch 4750 Loss 1.0636 Accuracy 0.4814\n",
      "Epoch 3 Batch 4800 Loss 1.0644 Accuracy 0.4813\n",
      "Epoch 3 Batch 4850 Loss 1.0655 Accuracy 0.4811\n",
      "Epoch 3 Batch 4900 Loss 1.0665 Accuracy 0.4809\n",
      "Epoch 3 Batch 4950 Loss 1.0674 Accuracy 0.4808\n",
      "Epoch 3 Batch 5000 Loss 1.0685 Accuracy 0.4806\n",
      "Epoch 3 Batch 5050 Loss 1.0695 Accuracy 0.4804\n",
      "Epoch 3 Batch 5100 Loss 1.0706 Accuracy 0.4802\n",
      "Epoch 3 Batch 5150 Loss 1.0717 Accuracy 0.4800\n",
      "Epoch 3 Batch 5200 Loss 1.0727 Accuracy 0.4798\n",
      "Epoch 3 Batch 5250 Loss 1.0739 Accuracy 0.4795\n",
      "Epoch 3 Batch 5300 Loss 1.0747 Accuracy 0.4793\n",
      "Epoch 3 Batch 5350 Loss 1.0758 Accuracy 0.4790\n",
      "Epoch 3 Batch 5400 Loss 1.0768 Accuracy 0.4788\n",
      "Epoch 3 Batch 5450 Loss 1.0775 Accuracy 0.4786\n",
      "Epoch 3 Batch 5500 Loss 1.0782 Accuracy 0.4783\n",
      "Epoch 3 Batch 5550 Loss 1.0790 Accuracy 0.4781\n",
      "Epoch 3 Batch 5600 Loss 1.0796 Accuracy 0.4780\n",
      "Epoch 3 Batch 5650 Loss 1.0804 Accuracy 0.4778\n",
      "Epoch 3 Batch 5700 Loss 1.0811 Accuracy 0.4776\n",
      "Saving checkpoint for epoch 3 at ./TF/ckpt/ckpt-4\n",
      "Time taken for 1 epoch: 1464.9874980449677 secs\n",
      "\n",
      "Start of epoch 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Batch 0 Loss 1.2725 Accuracy 0.4589\n",
      "Epoch 4 Batch 50 Loss 1.1762 Accuracy 0.4602\n",
      "Epoch 4 Batch 100 Loss 1.1564 Accuracy 0.4631\n",
      "Epoch 4 Batch 150 Loss 1.1564 Accuracy 0.4635\n",
      "Epoch 4 Batch 200 Loss 1.1514 Accuracy 0.4641\n",
      "Epoch 4 Batch 250 Loss 1.1481 Accuracy 0.4645\n",
      "Epoch 4 Batch 300 Loss 1.1501 Accuracy 0.4650\n",
      "Epoch 4 Batch 350 Loss 1.1493 Accuracy 0.4653\n",
      "Epoch 4 Batch 400 Loss 1.1469 Accuracy 0.4658\n",
      "Epoch 4 Batch 450 Loss 1.1460 Accuracy 0.4659\n",
      "Epoch 4 Batch 500 Loss 1.1448 Accuracy 0.4651\n",
      "Epoch 4 Batch 550 Loss 1.1453 Accuracy 0.4652\n",
      "Epoch 4 Batch 600 Loss 1.1419 Accuracy 0.4647\n",
      "Epoch 4 Batch 650 Loss 1.1421 Accuracy 0.4650\n",
      "Epoch 4 Batch 700 Loss 1.1404 Accuracy 0.4652\n",
      "Epoch 4 Batch 750 Loss 1.1407 Accuracy 0.4657\n",
      "Epoch 4 Batch 800 Loss 1.1405 Accuracy 0.4658\n",
      "Epoch 4 Batch 850 Loss 1.1417 Accuracy 0.4663\n",
      "Epoch 4 Batch 900 Loss 1.1401 Accuracy 0.4664\n",
      "Epoch 4 Batch 950 Loss 1.1366 Accuracy 0.4664\n",
      "Epoch 4 Batch 1000 Loss 1.1335 Accuracy 0.4666\n",
      "Epoch 4 Batch 1050 Loss 1.1325 Accuracy 0.4668\n",
      "Epoch 4 Batch 1100 Loss 1.1318 Accuracy 0.4669\n",
      "Epoch 4 Batch 1150 Loss 1.1311 Accuracy 0.4673\n",
      "Epoch 4 Batch 1200 Loss 1.1292 Accuracy 0.4674\n",
      "Epoch 4 Batch 1250 Loss 1.1265 Accuracy 0.4680\n",
      "Epoch 4 Batch 1300 Loss 1.1247 Accuracy 0.4682\n",
      "Epoch 4 Batch 1350 Loss 1.1224 Accuracy 0.4688\n",
      "Epoch 4 Batch 1400 Loss 1.1197 Accuracy 0.4696\n",
      "Epoch 4 Batch 1450 Loss 1.1173 Accuracy 0.4703\n",
      "Epoch 4 Batch 1500 Loss 1.1153 Accuracy 0.4711\n",
      "Epoch 4 Batch 1550 Loss 1.1123 Accuracy 0.4718\n",
      "Epoch 4 Batch 1600 Loss 1.1100 Accuracy 0.4729\n",
      "Epoch 4 Batch 1650 Loss 1.1070 Accuracy 0.4738\n",
      "Epoch 4 Batch 1700 Loss 1.1046 Accuracy 0.4746\n",
      "Epoch 4 Batch 1750 Loss 1.1026 Accuracy 0.4753\n",
      "Epoch 4 Batch 1800 Loss 1.1006 Accuracy 0.4761\n",
      "Epoch 4 Batch 1850 Loss 1.0984 Accuracy 0.4767\n",
      "Epoch 4 Batch 1900 Loss 1.0961 Accuracy 0.4774\n",
      "Epoch 4 Batch 1950 Loss 1.0944 Accuracy 0.4782\n",
      "Epoch 4 Batch 2000 Loss 1.0917 Accuracy 0.4788\n",
      "Epoch 4 Batch 2050 Loss 1.0893 Accuracy 0.4794\n",
      "Epoch 4 Batch 2100 Loss 1.0866 Accuracy 0.4798\n",
      "Epoch 4 Batch 2150 Loss 1.0840 Accuracy 0.4802\n",
      "Epoch 4 Batch 2200 Loss 1.0806 Accuracy 0.4803\n",
      "Epoch 4 Batch 2250 Loss 1.0777 Accuracy 0.4804\n",
      "Epoch 4 Batch 2300 Loss 1.0746 Accuracy 0.4806\n",
      "Epoch 4 Batch 2350 Loss 1.0717 Accuracy 0.4807\n",
      "Epoch 4 Batch 2400 Loss 1.0691 Accuracy 0.4810\n",
      "Epoch 4 Batch 2450 Loss 1.0661 Accuracy 0.4812\n",
      "Epoch 4 Batch 2500 Loss 1.0638 Accuracy 0.4815\n",
      "Epoch 4 Batch 2550 Loss 1.0611 Accuracy 0.4818\n",
      "Epoch 4 Batch 2600 Loss 1.0582 Accuracy 0.4821\n",
      "Epoch 4 Batch 2650 Loss 1.0554 Accuracy 0.4825\n",
      "Epoch 4 Batch 2700 Loss 1.0525 Accuracy 0.4829\n",
      "Epoch 4 Batch 2750 Loss 1.0496 Accuracy 0.4831\n",
      "Epoch 4 Batch 2800 Loss 1.0472 Accuracy 0.4835\n",
      "Epoch 4 Batch 2850 Loss 1.0449 Accuracy 0.4838\n",
      "Epoch 4 Batch 2900 Loss 1.0427 Accuracy 0.4841\n",
      "Epoch 4 Batch 2950 Loss 1.0408 Accuracy 0.4845\n",
      "Epoch 4 Batch 3000 Loss 1.0387 Accuracy 0.4848\n",
      "Epoch 4 Batch 3050 Loss 1.0365 Accuracy 0.4851\n",
      "Epoch 4 Batch 3100 Loss 1.0348 Accuracy 0.4854\n",
      "Epoch 4 Batch 3150 Loss 1.0326 Accuracy 0.4857\n",
      "Epoch 4 Batch 3200 Loss 1.0306 Accuracy 0.4858\n",
      "Epoch 4 Batch 3250 Loss 1.0286 Accuracy 0.4861\n",
      "Epoch 4 Batch 3300 Loss 1.0267 Accuracy 0.4863\n",
      "Epoch 4 Batch 3350 Loss 1.0248 Accuracy 0.4867\n",
      "Epoch 4 Batch 3400 Loss 1.0228 Accuracy 0.4869\n",
      "Epoch 4 Batch 3450 Loss 1.0204 Accuracy 0.4873\n",
      "Epoch 4 Batch 3500 Loss 1.0186 Accuracy 0.4876\n",
      "Epoch 4 Batch 3550 Loss 1.0166 Accuracy 0.4880\n",
      "Epoch 4 Batch 3600 Loss 1.0147 Accuracy 0.4883\n",
      "Epoch 4 Batch 3650 Loss 1.0128 Accuracy 0.4885\n",
      "Epoch 4 Batch 3700 Loss 1.0108 Accuracy 0.4889\n",
      "Epoch 4 Batch 3750 Loss 1.0093 Accuracy 0.4892\n",
      "Epoch 4 Batch 3800 Loss 1.0077 Accuracy 0.4895\n",
      "Epoch 4 Batch 3850 Loss 1.0065 Accuracy 0.4900\n",
      "Epoch 4 Batch 3900 Loss 1.0050 Accuracy 0.4903\n",
      "Epoch 4 Batch 3950 Loss 1.0034 Accuracy 0.4906\n",
      "Epoch 4 Batch 4000 Loss 1.0019 Accuracy 0.4910\n",
      "Epoch 4 Batch 4050 Loss 1.0006 Accuracy 0.4913\n",
      "Epoch 4 Batch 4100 Loss 0.9995 Accuracy 0.4915\n",
      "Epoch 4 Batch 4150 Loss 0.9990 Accuracy 0.4917\n",
      "Epoch 4 Batch 4200 Loss 0.9992 Accuracy 0.4917\n",
      "Epoch 4 Batch 4250 Loss 0.9994 Accuracy 0.4916\n",
      "Epoch 4 Batch 4300 Loss 1.0003 Accuracy 0.4915\n",
      "Epoch 4 Batch 4350 Loss 1.0012 Accuracy 0.4915\n",
      "Epoch 4 Batch 4400 Loss 1.0023 Accuracy 0.4913\n",
      "Epoch 4 Batch 4450 Loss 1.0031 Accuracy 0.4912\n",
      "Epoch 4 Batch 4500 Loss 1.0043 Accuracy 0.4910\n",
      "Epoch 4 Batch 4550 Loss 1.0058 Accuracy 0.4908\n",
      "Epoch 4 Batch 4600 Loss 1.0073 Accuracy 0.4906\n",
      "Epoch 4 Batch 4650 Loss 1.0085 Accuracy 0.4905\n",
      "Epoch 4 Batch 4700 Loss 1.0097 Accuracy 0.4903\n",
      "Epoch 4 Batch 4750 Loss 1.0108 Accuracy 0.4901\n",
      "Epoch 4 Batch 4800 Loss 1.0118 Accuracy 0.4900\n",
      "Epoch 4 Batch 4850 Loss 1.0129 Accuracy 0.4898\n",
      "Epoch 4 Batch 4900 Loss 1.0140 Accuracy 0.4897\n",
      "Epoch 4 Batch 4950 Loss 1.0152 Accuracy 0.4894\n",
      "Epoch 4 Batch 5000 Loss 1.0164 Accuracy 0.4892\n",
      "Epoch 4 Batch 5050 Loss 1.0175 Accuracy 0.4890\n",
      "Epoch 4 Batch 5100 Loss 1.0187 Accuracy 0.4887\n",
      "Epoch 4 Batch 5150 Loss 1.0198 Accuracy 0.4885\n",
      "Epoch 4 Batch 5200 Loss 1.0210 Accuracy 0.4883\n",
      "Epoch 4 Batch 5250 Loss 1.0223 Accuracy 0.4881\n",
      "Epoch 4 Batch 5300 Loss 1.0232 Accuracy 0.4878\n",
      "Epoch 4 Batch 5350 Loss 1.0240 Accuracy 0.4875\n",
      "Epoch 4 Batch 5400 Loss 1.0250 Accuracy 0.4872\n",
      "Epoch 4 Batch 5450 Loss 1.0259 Accuracy 0.4870\n",
      "Epoch 4 Batch 5500 Loss 1.0267 Accuracy 0.4868\n",
      "Epoch 4 Batch 5550 Loss 1.0277 Accuracy 0.4866\n",
      "Epoch 4 Batch 5600 Loss 1.0287 Accuracy 0.4864\n",
      "Epoch 4 Batch 5650 Loss 1.0294 Accuracy 0.4862\n",
      "Epoch 4 Batch 5700 Loss 1.0301 Accuracy 0.4859\n",
      "Saving checkpoint for epoch 4 at ./TF/ckpt/ckpt-5\n",
      "Time taken for 1 epoch: 1465.2272684574127 secs\n",
      "\n",
      "Start of epoch 5\n",
      "Epoch 5 Batch 0 Loss 1.1076 Accuracy 0.4753\n",
      "Epoch 5 Batch 50 Loss 1.1198 Accuracy 0.4732\n",
      "Epoch 5 Batch 100 Loss 1.1181 Accuracy 0.4742\n",
      "Epoch 5 Batch 150 Loss 1.1197 Accuracy 0.4728\n",
      "Epoch 5 Batch 200 Loss 1.1195 Accuracy 0.4726\n",
      "Epoch 5 Batch 250 Loss 1.1159 Accuracy 0.4717\n",
      "Epoch 5 Batch 300 Loss 1.1148 Accuracy 0.4716\n",
      "Epoch 5 Batch 350 Loss 1.1156 Accuracy 0.4720\n",
      "Epoch 5 Batch 400 Loss 1.1132 Accuracy 0.4719\n",
      "Epoch 5 Batch 450 Loss 1.1090 Accuracy 0.4717\n",
      "Epoch 5 Batch 500 Loss 1.1082 Accuracy 0.4719\n",
      "Epoch 5 Batch 550 Loss 1.1054 Accuracy 0.4717\n",
      "Epoch 5 Batch 600 Loss 1.1033 Accuracy 0.4715\n",
      "Epoch 5 Batch 650 Loss 1.1041 Accuracy 0.4720\n",
      "Epoch 5 Batch 700 Loss 1.1024 Accuracy 0.4724\n",
      "Epoch 5 Batch 750 Loss 1.1007 Accuracy 0.4726\n",
      "Epoch 5 Batch 800 Loss 1.1008 Accuracy 0.4727\n",
      "Epoch 5 Batch 850 Loss 1.1008 Accuracy 0.4725\n",
      "Epoch 5 Batch 900 Loss 1.0992 Accuracy 0.4726\n",
      "Epoch 5 Batch 950 Loss 1.0983 Accuracy 0.4727\n",
      "Epoch 5 Batch 1000 Loss 1.0962 Accuracy 0.4729\n",
      "Epoch 5 Batch 1050 Loss 1.0953 Accuracy 0.4729\n",
      "Epoch 5 Batch 1100 Loss 1.0939 Accuracy 0.4731\n",
      "Epoch 5 Batch 1150 Loss 1.0922 Accuracy 0.4733\n",
      "Epoch 5 Batch 1200 Loss 1.0906 Accuracy 0.4737\n",
      "Epoch 5 Batch 1250 Loss 1.0887 Accuracy 0.4739\n",
      "Epoch 5 Batch 1300 Loss 1.0866 Accuracy 0.4743\n",
      "Epoch 5 Batch 1350 Loss 1.0843 Accuracy 0.4749\n",
      "Epoch 5 Batch 1400 Loss 1.0814 Accuracy 0.4757\n",
      "Epoch 5 Batch 1450 Loss 1.0782 Accuracy 0.4763\n",
      "Epoch 5 Batch 1500 Loss 1.0747 Accuracy 0.4771\n",
      "Epoch 5 Batch 1550 Loss 1.0726 Accuracy 0.4779\n",
      "Epoch 5 Batch 1600 Loss 1.0699 Accuracy 0.4787\n",
      "Epoch 5 Batch 1650 Loss 1.0671 Accuracy 0.4796\n",
      "Epoch 5 Batch 1700 Loss 1.0650 Accuracy 0.4805\n",
      "Epoch 5 Batch 1750 Loss 1.0625 Accuracy 0.4813\n",
      "Epoch 5 Batch 1800 Loss 1.0603 Accuracy 0.4821\n",
      "Epoch 5 Batch 1850 Loss 1.0582 Accuracy 0.4828\n",
      "Epoch 5 Batch 1900 Loss 1.0558 Accuracy 0.4837\n",
      "Epoch 5 Batch 1950 Loss 1.0538 Accuracy 0.4844\n",
      "Epoch 5 Batch 2000 Loss 1.0515 Accuracy 0.4849\n",
      "Epoch 5 Batch 2050 Loss 1.0495 Accuracy 0.4854\n",
      "Epoch 5 Batch 2100 Loss 1.0466 Accuracy 0.4859\n",
      "Epoch 5 Batch 2150 Loss 1.0440 Accuracy 0.4864\n",
      "Epoch 5 Batch 2200 Loss 1.0416 Accuracy 0.4867\n",
      "Epoch 5 Batch 2250 Loss 1.0384 Accuracy 0.4868\n",
      "Epoch 5 Batch 2300 Loss 1.0352 Accuracy 0.4870\n",
      "Epoch 5 Batch 2350 Loss 1.0325 Accuracy 0.4872\n",
      "Epoch 5 Batch 2400 Loss 1.0299 Accuracy 0.4873\n",
      "Epoch 5 Batch 2450 Loss 1.0269 Accuracy 0.4876\n",
      "Epoch 5 Batch 2500 Loss 1.0237 Accuracy 0.4880\n",
      "Epoch 5 Batch 2550 Loss 1.0210 Accuracy 0.4884\n",
      "Epoch 5 Batch 2600 Loss 1.0183 Accuracy 0.4887\n",
      "Epoch 5 Batch 2650 Loss 1.0158 Accuracy 0.4891\n",
      "Epoch 5 Batch 2700 Loss 1.0127 Accuracy 0.4895\n",
      "Epoch 5 Batch 2750 Loss 1.0099 Accuracy 0.4899\n",
      "Epoch 5 Batch 2800 Loss 1.0077 Accuracy 0.4901\n",
      "Epoch 5 Batch 2850 Loss 1.0058 Accuracy 0.4905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Batch 2900 Loss 1.0037 Accuracy 0.4907\n",
      "Epoch 5 Batch 2950 Loss 1.0019 Accuracy 0.4909\n",
      "Epoch 5 Batch 3000 Loss 0.9997 Accuracy 0.4911\n",
      "Epoch 5 Batch 3050 Loss 0.9980 Accuracy 0.4914\n",
      "Epoch 5 Batch 3100 Loss 0.9960 Accuracy 0.4916\n",
      "Epoch 5 Batch 3150 Loss 0.9941 Accuracy 0.4918\n",
      "Epoch 5 Batch 3200 Loss 0.9922 Accuracy 0.4921\n",
      "Epoch 5 Batch 3250 Loss 0.9899 Accuracy 0.4924\n",
      "Epoch 5 Batch 3300 Loss 0.9878 Accuracy 0.4927\n",
      "Epoch 5 Batch 3350 Loss 0.9855 Accuracy 0.4930\n",
      "Epoch 5 Batch 3400 Loss 0.9835 Accuracy 0.4933\n",
      "Epoch 5 Batch 3450 Loss 0.9817 Accuracy 0.4936\n",
      "Epoch 5 Batch 3500 Loss 0.9798 Accuracy 0.4939\n",
      "Epoch 5 Batch 3550 Loss 0.9782 Accuracy 0.4943\n",
      "Epoch 5 Batch 3600 Loss 0.9761 Accuracy 0.4946\n",
      "Epoch 5 Batch 3650 Loss 0.9741 Accuracy 0.4949\n",
      "Epoch 5 Batch 3700 Loss 0.9728 Accuracy 0.4952\n",
      "Epoch 5 Batch 3750 Loss 0.9712 Accuracy 0.4956\n",
      "Epoch 5 Batch 3800 Loss 0.9697 Accuracy 0.4959\n",
      "Epoch 5 Batch 3850 Loss 0.9687 Accuracy 0.4962\n",
      "Epoch 5 Batch 3900 Loss 0.9671 Accuracy 0.4965\n",
      "Epoch 5 Batch 3950 Loss 0.9658 Accuracy 0.4969\n",
      "Epoch 5 Batch 4000 Loss 0.9645 Accuracy 0.4972\n",
      "Epoch 5 Batch 4050 Loss 0.9633 Accuracy 0.4975\n",
      "Epoch 5 Batch 4100 Loss 0.9621 Accuracy 0.4977\n",
      "Epoch 5 Batch 4150 Loss 0.9614 Accuracy 0.4979\n",
      "Epoch 5 Batch 4200 Loss 0.9618 Accuracy 0.4980\n",
      "Epoch 5 Batch 4250 Loss 0.9620 Accuracy 0.4979\n",
      "Epoch 5 Batch 4300 Loss 0.9628 Accuracy 0.4978\n",
      "Epoch 5 Batch 4350 Loss 0.9636 Accuracy 0.4977\n",
      "Epoch 5 Batch 4400 Loss 0.9645 Accuracy 0.4975\n",
      "Epoch 5 Batch 4450 Loss 0.9659 Accuracy 0.4973\n",
      "Epoch 5 Batch 4500 Loss 0.9672 Accuracy 0.4972\n",
      "Epoch 5 Batch 4550 Loss 0.9685 Accuracy 0.4970\n",
      "Epoch 5 Batch 4600 Loss 0.9698 Accuracy 0.4968\n",
      "Epoch 5 Batch 4650 Loss 0.9711 Accuracy 0.4966\n",
      "Epoch 5 Batch 4700 Loss 0.9721 Accuracy 0.4965\n",
      "Epoch 5 Batch 4750 Loss 0.9735 Accuracy 0.4963\n",
      "Epoch 5 Batch 4800 Loss 0.9747 Accuracy 0.4961\n",
      "Epoch 5 Batch 4850 Loss 0.9759 Accuracy 0.4959\n",
      "Epoch 5 Batch 4900 Loss 0.9771 Accuracy 0.4958\n",
      "Epoch 5 Batch 4950 Loss 0.9780 Accuracy 0.4956\n",
      "Epoch 5 Batch 5000 Loss 0.9792 Accuracy 0.4954\n",
      "Epoch 5 Batch 5050 Loss 0.9805 Accuracy 0.4951\n",
      "Epoch 5 Batch 5100 Loss 0.9817 Accuracy 0.4949\n",
      "Epoch 5 Batch 5150 Loss 0.9831 Accuracy 0.4946\n",
      "Epoch 5 Batch 5200 Loss 0.9841 Accuracy 0.4944\n",
      "Epoch 5 Batch 5250 Loss 0.9854 Accuracy 0.4941\n",
      "Epoch 5 Batch 5300 Loss 0.9864 Accuracy 0.4939\n",
      "Epoch 5 Batch 5350 Loss 0.9875 Accuracy 0.4936\n",
      "Epoch 5 Batch 5400 Loss 0.9883 Accuracy 0.4933\n",
      "Epoch 5 Batch 5450 Loss 0.9892 Accuracy 0.4931\n",
      "Epoch 5 Batch 5500 Loss 0.9904 Accuracy 0.4928\n",
      "Epoch 5 Batch 5550 Loss 0.9915 Accuracy 0.4926\n",
      "Epoch 5 Batch 5600 Loss 0.9923 Accuracy 0.4923\n",
      "Epoch 5 Batch 5650 Loss 0.9932 Accuracy 0.4921\n",
      "Epoch 5 Batch 5700 Loss 0.9943 Accuracy 0.4919\n",
      "Saving checkpoint for epoch 5 at ./TF/ckpt/ckpt-6\n",
      "Time taken for 1 epoch: 1466.4269659519196 secs\n",
      "\n",
      "Start of epoch 6\n",
      "Epoch 6 Batch 0 Loss 1.0046 Accuracy 0.4646\n",
      "Epoch 6 Batch 50 Loss 1.0766 Accuracy 0.4773\n",
      "Epoch 6 Batch 100 Loss 1.0836 Accuracy 0.4779\n",
      "Epoch 6 Batch 150 Loss 1.0777 Accuracy 0.4774\n",
      "Epoch 6 Batch 200 Loss 1.0801 Accuracy 0.4768\n",
      "Epoch 6 Batch 250 Loss 1.0793 Accuracy 0.4769\n",
      "Epoch 6 Batch 300 Loss 1.0794 Accuracy 0.4764\n",
      "Epoch 6 Batch 350 Loss 1.0800 Accuracy 0.4764\n",
      "Epoch 6 Batch 400 Loss 1.0785 Accuracy 0.4772\n",
      "Epoch 6 Batch 450 Loss 1.0782 Accuracy 0.4769\n",
      "Epoch 6 Batch 500 Loss 1.0758 Accuracy 0.4764\n",
      "Epoch 6 Batch 550 Loss 1.0750 Accuracy 0.4765\n",
      "Epoch 6 Batch 600 Loss 1.0743 Accuracy 0.4768\n",
      "Epoch 6 Batch 650 Loss 1.0747 Accuracy 0.4773\n",
      "Epoch 6 Batch 700 Loss 1.0744 Accuracy 0.4776\n",
      "Epoch 6 Batch 750 Loss 1.0738 Accuracy 0.4780\n",
      "Epoch 6 Batch 800 Loss 1.0739 Accuracy 0.4778\n",
      "Epoch 6 Batch 850 Loss 1.0723 Accuracy 0.4783\n",
      "Epoch 6 Batch 900 Loss 1.0703 Accuracy 0.4781\n",
      "Epoch 6 Batch 950 Loss 1.0683 Accuracy 0.4781\n",
      "Epoch 6 Batch 1000 Loss 1.0664 Accuracy 0.4782\n",
      "Epoch 6 Batch 1050 Loss 1.0648 Accuracy 0.4783\n",
      "Epoch 6 Batch 1100 Loss 1.0630 Accuracy 0.4785\n",
      "Epoch 6 Batch 1150 Loss 1.0620 Accuracy 0.4786\n",
      "Epoch 6 Batch 1200 Loss 1.0611 Accuracy 0.4789\n",
      "Epoch 6 Batch 1250 Loss 1.0595 Accuracy 0.4791\n",
      "Epoch 6 Batch 1300 Loss 1.0574 Accuracy 0.4797\n",
      "Epoch 6 Batch 1350 Loss 1.0549 Accuracy 0.4803\n",
      "Epoch 6 Batch 1400 Loss 1.0520 Accuracy 0.4810\n",
      "Epoch 6 Batch 1450 Loss 1.0494 Accuracy 0.4817\n",
      "Epoch 6 Batch 1500 Loss 1.0458 Accuracy 0.4824\n",
      "Epoch 6 Batch 1550 Loss 1.0427 Accuracy 0.4833\n",
      "Epoch 6 Batch 1600 Loss 1.0398 Accuracy 0.4842\n",
      "Epoch 6 Batch 1650 Loss 1.0373 Accuracy 0.4849\n",
      "Epoch 6 Batch 1700 Loss 1.0347 Accuracy 0.4859\n",
      "Epoch 6 Batch 1750 Loss 1.0327 Accuracy 0.4866\n",
      "Epoch 6 Batch 1800 Loss 1.0307 Accuracy 0.4874\n",
      "Epoch 6 Batch 1850 Loss 1.0283 Accuracy 0.4883\n",
      "Epoch 6 Batch 1900 Loss 1.0263 Accuracy 0.4889\n",
      "Epoch 6 Batch 1950 Loss 1.0242 Accuracy 0.4896\n",
      "Epoch 6 Batch 2000 Loss 1.0226 Accuracy 0.4900\n",
      "Epoch 6 Batch 2050 Loss 1.0210 Accuracy 0.4906\n",
      "Epoch 6 Batch 2100 Loss 1.0183 Accuracy 0.4909\n",
      "Epoch 6 Batch 2150 Loss 1.0152 Accuracy 0.4913\n",
      "Epoch 6 Batch 2200 Loss 1.0120 Accuracy 0.4914\n",
      "Epoch 6 Batch 2250 Loss 1.0086 Accuracy 0.4916\n",
      "Epoch 6 Batch 2300 Loss 1.0056 Accuracy 0.4918\n",
      "Epoch 6 Batch 2350 Loss 1.0030 Accuracy 0.4921\n",
      "Epoch 6 Batch 2400 Loss 1.0002 Accuracy 0.4923\n",
      "Epoch 6 Batch 2450 Loss 0.9970 Accuracy 0.4926\n",
      "Epoch 6 Batch 2500 Loss 0.9943 Accuracy 0.4928\n",
      "Epoch 6 Batch 2550 Loss 0.9912 Accuracy 0.4931\n",
      "Epoch 6 Batch 2600 Loss 0.9887 Accuracy 0.4935\n",
      "Epoch 6 Batch 2650 Loss 0.9860 Accuracy 0.4937\n",
      "Epoch 6 Batch 2700 Loss 0.9832 Accuracy 0.4942\n",
      "Epoch 6 Batch 2750 Loss 0.9811 Accuracy 0.4945\n",
      "Epoch 6 Batch 2800 Loss 0.9791 Accuracy 0.4948\n",
      "Epoch 6 Batch 2850 Loss 0.9771 Accuracy 0.4951\n",
      "Epoch 6 Batch 2900 Loss 0.9747 Accuracy 0.4954\n",
      "Epoch 6 Batch 2950 Loss 0.9726 Accuracy 0.4958\n",
      "Epoch 6 Batch 3000 Loss 0.9702 Accuracy 0.4960\n",
      "Epoch 6 Batch 3050 Loss 0.9682 Accuracy 0.4961\n",
      "Epoch 6 Batch 3100 Loss 0.9666 Accuracy 0.4964\n",
      "Epoch 6 Batch 3150 Loss 0.9649 Accuracy 0.4967\n",
      "Epoch 6 Batch 3200 Loss 0.9626 Accuracy 0.4969\n",
      "Epoch 6 Batch 3250 Loss 0.9607 Accuracy 0.4971\n",
      "Epoch 6 Batch 3300 Loss 0.9585 Accuracy 0.4975\n",
      "Epoch 6 Batch 3350 Loss 0.9567 Accuracy 0.4977\n",
      "Epoch 6 Batch 3400 Loss 0.9549 Accuracy 0.4980\n",
      "Epoch 6 Batch 3450 Loss 0.9532 Accuracy 0.4983\n",
      "Epoch 6 Batch 3500 Loss 0.9512 Accuracy 0.4986\n",
      "Epoch 6 Batch 3550 Loss 0.9497 Accuracy 0.4989\n",
      "Epoch 6 Batch 3600 Loss 0.9481 Accuracy 0.4992\n",
      "Epoch 6 Batch 3650 Loss 0.9464 Accuracy 0.4996\n",
      "Epoch 6 Batch 3700 Loss 0.9448 Accuracy 0.5000\n",
      "Epoch 6 Batch 3750 Loss 0.9432 Accuracy 0.5002\n",
      "Epoch 6 Batch 3800 Loss 0.9418 Accuracy 0.5004\n",
      "Epoch 6 Batch 3850 Loss 0.9403 Accuracy 0.5007\n",
      "Epoch 6 Batch 3900 Loss 0.9388 Accuracy 0.5010\n",
      "Epoch 6 Batch 3950 Loss 0.9372 Accuracy 0.5014\n",
      "Epoch 6 Batch 4000 Loss 0.9359 Accuracy 0.5017\n",
      "Epoch 6 Batch 4050 Loss 0.9349 Accuracy 0.5021\n",
      "Epoch 6 Batch 4100 Loss 0.9339 Accuracy 0.5023\n",
      "Epoch 6 Batch 4150 Loss 0.9333 Accuracy 0.5023\n",
      "Epoch 6 Batch 4200 Loss 0.9333 Accuracy 0.5024\n",
      "Epoch 6 Batch 4250 Loss 0.9338 Accuracy 0.5024\n",
      "Epoch 6 Batch 4300 Loss 0.9349 Accuracy 0.5023\n",
      "Epoch 6 Batch 4350 Loss 0.9359 Accuracy 0.5021\n",
      "Epoch 6 Batch 4400 Loss 0.9369 Accuracy 0.5020\n",
      "Epoch 6 Batch 4450 Loss 0.9382 Accuracy 0.5018\n",
      "Epoch 6 Batch 4500 Loss 0.9393 Accuracy 0.5016\n",
      "Epoch 6 Batch 4550 Loss 0.9407 Accuracy 0.5014\n",
      "Epoch 6 Batch 4600 Loss 0.9423 Accuracy 0.5012\n",
      "Epoch 6 Batch 4650 Loss 0.9436 Accuracy 0.5010\n",
      "Epoch 6 Batch 4700 Loss 0.9449 Accuracy 0.5009\n",
      "Epoch 6 Batch 4750 Loss 0.9460 Accuracy 0.5007\n",
      "Epoch 6 Batch 4800 Loss 0.9471 Accuracy 0.5005\n",
      "Epoch 6 Batch 4850 Loss 0.9484 Accuracy 0.5004\n",
      "Epoch 6 Batch 4900 Loss 0.9495 Accuracy 0.5002\n",
      "Epoch 6 Batch 4950 Loss 0.9508 Accuracy 0.5000\n",
      "Epoch 6 Batch 5000 Loss 0.9521 Accuracy 0.4998\n",
      "Epoch 6 Batch 5050 Loss 0.9533 Accuracy 0.4995\n",
      "Epoch 6 Batch 5100 Loss 0.9547 Accuracy 0.4993\n",
      "Epoch 6 Batch 5150 Loss 0.9560 Accuracy 0.4991\n",
      "Epoch 6 Batch 5200 Loss 0.9573 Accuracy 0.4988\n",
      "Epoch 6 Batch 5250 Loss 0.9584 Accuracy 0.4985\n",
      "Epoch 6 Batch 5300 Loss 0.9595 Accuracy 0.4983\n",
      "Epoch 6 Batch 5350 Loss 0.9607 Accuracy 0.4980\n",
      "Epoch 6 Batch 5400 Loss 0.9617 Accuracy 0.4977\n",
      "Epoch 6 Batch 5450 Loss 0.9627 Accuracy 0.4974\n",
      "Epoch 6 Batch 5500 Loss 0.9637 Accuracy 0.4972\n",
      "Epoch 6 Batch 5550 Loss 0.9645 Accuracy 0.4969\n",
      "Epoch 6 Batch 5600 Loss 0.9653 Accuracy 0.4967\n",
      "Epoch 6 Batch 5650 Loss 0.9664 Accuracy 0.4965\n",
      "Epoch 6 Batch 5700 Loss 0.9674 Accuracy 0.4963\n",
      "Saving checkpoint for epoch 6 at ./TF/ckpt/ckpt-7\n",
      "Time taken for 1 epoch: 1460.9730470180511 secs\n",
      "\n",
      "Start of epoch 7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Batch 0 Loss 1.1024 Accuracy 0.4137\n",
      "Epoch 7 Batch 50 Loss 1.0605 Accuracy 0.4840\n",
      "Epoch 7 Batch 100 Loss 1.0764 Accuracy 0.4801\n",
      "Epoch 7 Batch 150 Loss 1.0667 Accuracy 0.4798\n",
      "Epoch 7 Batch 200 Loss 1.0601 Accuracy 0.4795\n",
      "Epoch 7 Batch 250 Loss 1.0629 Accuracy 0.4801\n",
      "Epoch 7 Batch 300 Loss 1.0624 Accuracy 0.4804\n",
      "Epoch 7 Batch 350 Loss 1.0594 Accuracy 0.4806\n",
      "Epoch 7 Batch 400 Loss 1.0593 Accuracy 0.4806\n",
      "Epoch 7 Batch 450 Loss 1.0550 Accuracy 0.4806\n",
      "Epoch 7 Batch 500 Loss 1.0531 Accuracy 0.4812\n",
      "Epoch 7 Batch 550 Loss 1.0502 Accuracy 0.4810\n",
      "Epoch 7 Batch 600 Loss 1.0474 Accuracy 0.4812\n",
      "Epoch 7 Batch 650 Loss 1.0475 Accuracy 0.4815\n",
      "Epoch 7 Batch 700 Loss 1.0472 Accuracy 0.4814\n",
      "Epoch 7 Batch 750 Loss 1.0462 Accuracy 0.4818\n",
      "Epoch 7 Batch 800 Loss 1.0442 Accuracy 0.4819\n",
      "Epoch 7 Batch 850 Loss 1.0437 Accuracy 0.4817\n",
      "Epoch 7 Batch 900 Loss 1.0424 Accuracy 0.4820\n",
      "Epoch 7 Batch 950 Loss 1.0416 Accuracy 0.4822\n",
      "Epoch 7 Batch 1000 Loss 1.0404 Accuracy 0.4823\n",
      "Epoch 7 Batch 1050 Loss 1.0386 Accuracy 0.4822\n",
      "Epoch 7 Batch 1100 Loss 1.0373 Accuracy 0.4821\n",
      "Epoch 7 Batch 1150 Loss 1.0361 Accuracy 0.4825\n",
      "Epoch 7 Batch 1200 Loss 1.0342 Accuracy 0.4830\n",
      "Epoch 7 Batch 1250 Loss 1.0326 Accuracy 0.4833\n",
      "Epoch 7 Batch 1300 Loss 1.0307 Accuracy 0.4836\n",
      "Epoch 7 Batch 1350 Loss 1.0288 Accuracy 0.4840\n",
      "Epoch 7 Batch 1400 Loss 1.0256 Accuracy 0.4848\n",
      "Epoch 7 Batch 1450 Loss 1.0234 Accuracy 0.4854\n",
      "Epoch 7 Batch 1500 Loss 1.0212 Accuracy 0.4864\n",
      "Epoch 7 Batch 1550 Loss 1.0187 Accuracy 0.4871\n",
      "Epoch 7 Batch 1600 Loss 1.0159 Accuracy 0.4879\n",
      "Epoch 7 Batch 1650 Loss 1.0136 Accuracy 0.4888\n",
      "Epoch 7 Batch 1700 Loss 1.0114 Accuracy 0.4897\n",
      "Epoch 7 Batch 1750 Loss 1.0091 Accuracy 0.4904\n",
      "Epoch 7 Batch 1800 Loss 1.0066 Accuracy 0.4912\n",
      "Epoch 7 Batch 1850 Loss 1.0048 Accuracy 0.4919\n",
      "Epoch 7 Batch 1900 Loss 1.0027 Accuracy 0.4927\n",
      "Epoch 7 Batch 1950 Loss 1.0005 Accuracy 0.4934\n",
      "Epoch 7 Batch 2000 Loss 0.9989 Accuracy 0.4940\n",
      "Epoch 7 Batch 2050 Loss 0.9968 Accuracy 0.4945\n",
      "Epoch 7 Batch 2100 Loss 0.9941 Accuracy 0.4949\n",
      "Epoch 7 Batch 2150 Loss 0.9915 Accuracy 0.4953\n",
      "Epoch 7 Batch 2200 Loss 0.9882 Accuracy 0.4954\n",
      "Epoch 7 Batch 2250 Loss 0.9852 Accuracy 0.4955\n",
      "Epoch 7 Batch 2300 Loss 0.9821 Accuracy 0.4956\n",
      "Epoch 7 Batch 2350 Loss 0.9794 Accuracy 0.4959\n",
      "Epoch 7 Batch 2400 Loss 0.9769 Accuracy 0.4960\n",
      "Epoch 7 Batch 2450 Loss 0.9739 Accuracy 0.4962\n",
      "Epoch 7 Batch 2500 Loss 0.9712 Accuracy 0.4966\n",
      "Epoch 7 Batch 2550 Loss 0.9686 Accuracy 0.4970\n",
      "Epoch 7 Batch 2600 Loss 0.9661 Accuracy 0.4972\n",
      "Epoch 7 Batch 2650 Loss 0.9637 Accuracy 0.4974\n",
      "Epoch 7 Batch 2700 Loss 0.9614 Accuracy 0.4978\n",
      "Epoch 7 Batch 2750 Loss 0.9600 Accuracy 0.4982\n",
      "Epoch 7 Batch 2800 Loss 0.9577 Accuracy 0.4985\n",
      "Epoch 7 Batch 2850 Loss 0.9552 Accuracy 0.4988\n",
      "Epoch 7 Batch 2900 Loss 0.9530 Accuracy 0.4990\n",
      "Epoch 7 Batch 2950 Loss 0.9507 Accuracy 0.4993\n",
      "Epoch 7 Batch 3000 Loss 0.9488 Accuracy 0.4996\n",
      "Epoch 7 Batch 3050 Loss 0.9472 Accuracy 0.4999\n",
      "Epoch 7 Batch 3100 Loss 0.9452 Accuracy 0.5001\n",
      "Epoch 7 Batch 3150 Loss 0.9433 Accuracy 0.5003\n",
      "Epoch 7 Batch 3200 Loss 0.9411 Accuracy 0.5005\n",
      "Epoch 7 Batch 3250 Loss 0.9395 Accuracy 0.5008\n",
      "Epoch 7 Batch 3300 Loss 0.9378 Accuracy 0.5011\n",
      "Epoch 7 Batch 3350 Loss 0.9357 Accuracy 0.5014\n",
      "Epoch 7 Batch 3400 Loss 0.9337 Accuracy 0.5017\n",
      "Epoch 7 Batch 3450 Loss 0.9322 Accuracy 0.5019\n",
      "Epoch 7 Batch 3500 Loss 0.9305 Accuracy 0.5023\n",
      "Epoch 7 Batch 3550 Loss 0.9285 Accuracy 0.5026\n",
      "Epoch 7 Batch 3600 Loss 0.9268 Accuracy 0.5027\n",
      "Epoch 7 Batch 3650 Loss 0.9252 Accuracy 0.5031\n",
      "Epoch 7 Batch 3700 Loss 0.9234 Accuracy 0.5034\n",
      "Epoch 7 Batch 3750 Loss 0.9217 Accuracy 0.5037\n",
      "Epoch 7 Batch 3800 Loss 0.9205 Accuracy 0.5040\n",
      "Epoch 7 Batch 3850 Loss 0.9189 Accuracy 0.5043\n",
      "Epoch 7 Batch 3900 Loss 0.9175 Accuracy 0.5047\n",
      "Epoch 7 Batch 3950 Loss 0.9162 Accuracy 0.5050\n",
      "Epoch 7 Batch 4000 Loss 0.9149 Accuracy 0.5052\n",
      "Epoch 7 Batch 4050 Loss 0.9139 Accuracy 0.5055\n",
      "Epoch 7 Batch 4100 Loss 0.9127 Accuracy 0.5058\n",
      "Epoch 7 Batch 4150 Loss 0.9123 Accuracy 0.5059\n",
      "Epoch 7 Batch 4200 Loss 0.9122 Accuracy 0.5059\n",
      "Epoch 7 Batch 4250 Loss 0.9128 Accuracy 0.5059\n",
      "Epoch 7 Batch 4300 Loss 0.9136 Accuracy 0.5057\n",
      "Epoch 7 Batch 4350 Loss 0.9148 Accuracy 0.5056\n",
      "Epoch 7 Batch 4400 Loss 0.9158 Accuracy 0.5054\n",
      "Epoch 7 Batch 4450 Loss 0.9171 Accuracy 0.5052\n",
      "Epoch 7 Batch 4500 Loss 0.9186 Accuracy 0.5050\n",
      "Epoch 7 Batch 4550 Loss 0.9200 Accuracy 0.5048\n",
      "Epoch 7 Batch 4600 Loss 0.9214 Accuracy 0.5046\n",
      "Epoch 7 Batch 4650 Loss 0.9226 Accuracy 0.5044\n",
      "Epoch 7 Batch 4700 Loss 0.9239 Accuracy 0.5042\n",
      "Epoch 7 Batch 4750 Loss 0.9251 Accuracy 0.5041\n",
      "Epoch 7 Batch 4800 Loss 0.9262 Accuracy 0.5039\n",
      "Epoch 7 Batch 4850 Loss 0.9274 Accuracy 0.5037\n",
      "Epoch 7 Batch 4900 Loss 0.9287 Accuracy 0.5035\n",
      "Epoch 7 Batch 4950 Loss 0.9298 Accuracy 0.5034\n",
      "Epoch 7 Batch 5000 Loss 0.9311 Accuracy 0.5032\n",
      "Epoch 7 Batch 5050 Loss 0.9321 Accuracy 0.5030\n",
      "Epoch 7 Batch 5100 Loss 0.9336 Accuracy 0.5028\n",
      "Epoch 7 Batch 5150 Loss 0.9350 Accuracy 0.5025\n",
      "Epoch 7 Batch 5200 Loss 0.9365 Accuracy 0.5024\n",
      "Epoch 7 Batch 5250 Loss 0.9377 Accuracy 0.5021\n",
      "Epoch 7 Batch 5300 Loss 0.9389 Accuracy 0.5018\n",
      "Epoch 7 Batch 5350 Loss 0.9401 Accuracy 0.5015\n",
      "Epoch 7 Batch 5400 Loss 0.9412 Accuracy 0.5012\n",
      "Epoch 7 Batch 5450 Loss 0.9422 Accuracy 0.5010\n",
      "Epoch 7 Batch 5500 Loss 0.9430 Accuracy 0.5007\n",
      "Epoch 7 Batch 5550 Loss 0.9441 Accuracy 0.5004\n",
      "Epoch 7 Batch 5600 Loss 0.9451 Accuracy 0.5001\n",
      "Epoch 7 Batch 5650 Loss 0.9461 Accuracy 0.4999\n",
      "Epoch 7 Batch 5700 Loss 0.9469 Accuracy 0.4996\n",
      "Saving checkpoint for epoch 7 at ./TF/ckpt/ckpt-8\n",
      "Time taken for 1 epoch: 1465.7228047847748 secs\n",
      "\n",
      "Start of epoch 8\n",
      "Epoch 8 Batch 0 Loss 1.0032 Accuracy 0.4400\n",
      "Epoch 8 Batch 50 Loss 1.0290 Accuracy 0.4818\n",
      "Epoch 8 Batch 100 Loss 1.0370 Accuracy 0.4853\n",
      "Epoch 8 Batch 150 Loss 1.0419 Accuracy 0.4849\n",
      "Epoch 8 Batch 200 Loss 1.0397 Accuracy 0.4856\n",
      "Epoch 8 Batch 250 Loss 1.0431 Accuracy 0.4846\n",
      "Epoch 8 Batch 300 Loss 1.0393 Accuracy 0.4845\n",
      "Epoch 8 Batch 350 Loss 1.0376 Accuracy 0.4841\n",
      "Epoch 8 Batch 400 Loss 1.0393 Accuracy 0.4836\n",
      "Epoch 8 Batch 450 Loss 1.0330 Accuracy 0.4837\n",
      "Epoch 8 Batch 500 Loss 1.0327 Accuracy 0.4838\n",
      "Epoch 8 Batch 550 Loss 1.0322 Accuracy 0.4839\n",
      "Epoch 8 Batch 600 Loss 1.0324 Accuracy 0.4842\n",
      "Epoch 8 Batch 650 Loss 1.0323 Accuracy 0.4845\n",
      "Epoch 8 Batch 700 Loss 1.0306 Accuracy 0.4850\n",
      "Epoch 8 Batch 750 Loss 1.0300 Accuracy 0.4851\n",
      "Epoch 8 Batch 800 Loss 1.0294 Accuracy 0.4851\n",
      "Epoch 8 Batch 850 Loss 1.0289 Accuracy 0.4849\n",
      "Epoch 8 Batch 900 Loss 1.0258 Accuracy 0.4849\n",
      "Epoch 8 Batch 950 Loss 1.0254 Accuracy 0.4851\n",
      "Epoch 8 Batch 1000 Loss 1.0235 Accuracy 0.4853\n",
      "Epoch 8 Batch 1050 Loss 1.0220 Accuracy 0.4854\n",
      "Epoch 8 Batch 1100 Loss 1.0206 Accuracy 0.4855\n",
      "Epoch 8 Batch 1150 Loss 1.0184 Accuracy 0.4857\n",
      "Epoch 8 Batch 1200 Loss 1.0170 Accuracy 0.4859\n",
      "Epoch 8 Batch 1250 Loss 1.0143 Accuracy 0.4861\n",
      "Epoch 8 Batch 1300 Loss 1.0127 Accuracy 0.4864\n",
      "Epoch 8 Batch 1350 Loss 1.0110 Accuracy 0.4871\n",
      "Epoch 8 Batch 1400 Loss 1.0087 Accuracy 0.4878\n",
      "Epoch 8 Batch 1450 Loss 1.0062 Accuracy 0.4882\n",
      "Epoch 8 Batch 1500 Loss 1.0031 Accuracy 0.4890\n",
      "Epoch 8 Batch 1550 Loss 1.0004 Accuracy 0.4897\n",
      "Epoch 8 Batch 1600 Loss 0.9977 Accuracy 0.4906\n",
      "Epoch 8 Batch 1650 Loss 0.9957 Accuracy 0.4915\n",
      "Epoch 8 Batch 1700 Loss 0.9937 Accuracy 0.4922\n",
      "Epoch 8 Batch 1750 Loss 0.9913 Accuracy 0.4930\n",
      "Epoch 8 Batch 1800 Loss 0.9886 Accuracy 0.4939\n",
      "Epoch 8 Batch 1850 Loss 0.9858 Accuracy 0.4948\n",
      "Epoch 8 Batch 1900 Loss 0.9835 Accuracy 0.4956\n",
      "Epoch 8 Batch 1950 Loss 0.9815 Accuracy 0.4965\n",
      "Epoch 8 Batch 2000 Loss 0.9798 Accuracy 0.4972\n",
      "Epoch 8 Batch 2050 Loss 0.9778 Accuracy 0.4977\n",
      "Epoch 8 Batch 2100 Loss 0.9759 Accuracy 0.4980\n",
      "Epoch 8 Batch 2150 Loss 0.9733 Accuracy 0.4984\n",
      "Epoch 8 Batch 2200 Loss 0.9708 Accuracy 0.4985\n",
      "Epoch 8 Batch 2250 Loss 0.9675 Accuracy 0.4987\n",
      "Epoch 8 Batch 2300 Loss 0.9648 Accuracy 0.4988\n",
      "Epoch 8 Batch 2350 Loss 0.9619 Accuracy 0.4989\n",
      "Epoch 8 Batch 2400 Loss 0.9592 Accuracy 0.4991\n",
      "Epoch 8 Batch 2450 Loss 0.9564 Accuracy 0.4993\n",
      "Epoch 8 Batch 2500 Loss 0.9535 Accuracy 0.4998\n",
      "Epoch 8 Batch 2550 Loss 0.9508 Accuracy 0.5001\n",
      "Epoch 8 Batch 2600 Loss 0.9483 Accuracy 0.5004\n",
      "Epoch 8 Batch 2650 Loss 0.9457 Accuracy 0.5008\n",
      "Epoch 8 Batch 2700 Loss 0.9433 Accuracy 0.5011\n",
      "Epoch 8 Batch 2750 Loss 0.9406 Accuracy 0.5015\n",
      "Epoch 8 Batch 2800 Loss 0.9385 Accuracy 0.5017\n",
      "Epoch 8 Batch 2850 Loss 0.9360 Accuracy 0.5019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 2900 Loss 0.9343 Accuracy 0.5022\n",
      "Epoch 8 Batch 2950 Loss 0.9323 Accuracy 0.5025\n",
      "Epoch 8 Batch 3000 Loss 0.9306 Accuracy 0.5027\n",
      "Epoch 8 Batch 3050 Loss 0.9290 Accuracy 0.5030\n",
      "Epoch 8 Batch 3100 Loss 0.9270 Accuracy 0.5032\n",
      "Epoch 8 Batch 3150 Loss 0.9253 Accuracy 0.5034\n",
      "Epoch 8 Batch 3200 Loss 0.9236 Accuracy 0.5036\n",
      "Epoch 8 Batch 3250 Loss 0.9216 Accuracy 0.5038\n",
      "Epoch 8 Batch 3300 Loss 0.9196 Accuracy 0.5041\n",
      "Epoch 8 Batch 3350 Loss 0.9174 Accuracy 0.5043\n",
      "Epoch 8 Batch 3400 Loss 0.9155 Accuracy 0.5046\n",
      "Epoch 8 Batch 3450 Loss 0.9137 Accuracy 0.5049\n",
      "Epoch 8 Batch 3500 Loss 0.9121 Accuracy 0.5052\n",
      "Epoch 8 Batch 3550 Loss 0.9106 Accuracy 0.5056\n",
      "Epoch 8 Batch 3600 Loss 0.9087 Accuracy 0.5059\n",
      "Epoch 8 Batch 3650 Loss 0.9073 Accuracy 0.5062\n",
      "Epoch 8 Batch 3700 Loss 0.9058 Accuracy 0.5065\n",
      "Epoch 8 Batch 3750 Loss 0.9044 Accuracy 0.5068\n",
      "Epoch 8 Batch 3800 Loss 0.9029 Accuracy 0.5072\n",
      "Epoch 8 Batch 3850 Loss 0.9016 Accuracy 0.5074\n",
      "Epoch 8 Batch 3900 Loss 0.9001 Accuracy 0.5078\n",
      "Epoch 8 Batch 3950 Loss 0.8987 Accuracy 0.5082\n",
      "Epoch 8 Batch 4000 Loss 0.8972 Accuracy 0.5084\n",
      "Epoch 8 Batch 4050 Loss 0.8959 Accuracy 0.5088\n",
      "Epoch 8 Batch 4100 Loss 0.8949 Accuracy 0.5089\n",
      "Epoch 8 Batch 4150 Loss 0.8944 Accuracy 0.5091\n",
      "Epoch 8 Batch 4200 Loss 0.8945 Accuracy 0.5091\n",
      "Epoch 8 Batch 4250 Loss 0.8951 Accuracy 0.5090\n",
      "Epoch 8 Batch 4300 Loss 0.8963 Accuracy 0.5089\n",
      "Epoch 8 Batch 4350 Loss 0.8971 Accuracy 0.5088\n",
      "Epoch 8 Batch 4400 Loss 0.8980 Accuracy 0.5086\n",
      "Epoch 8 Batch 4450 Loss 0.8991 Accuracy 0.5084\n",
      "Epoch 8 Batch 4500 Loss 0.9004 Accuracy 0.5082\n",
      "Epoch 8 Batch 4550 Loss 0.9017 Accuracy 0.5080\n",
      "Epoch 8 Batch 4600 Loss 0.9033 Accuracy 0.5077\n",
      "Epoch 8 Batch 4650 Loss 0.9045 Accuracy 0.5076\n",
      "Epoch 8 Batch 4700 Loss 0.9059 Accuracy 0.5074\n",
      "Epoch 8 Batch 4750 Loss 0.9071 Accuracy 0.5072\n",
      "Epoch 8 Batch 4800 Loss 0.9085 Accuracy 0.5070\n",
      "Epoch 8 Batch 4850 Loss 0.9096 Accuracy 0.5069\n",
      "Epoch 8 Batch 4900 Loss 0.9110 Accuracy 0.5067\n",
      "Epoch 8 Batch 4950 Loss 0.9122 Accuracy 0.5065\n",
      "Epoch 8 Batch 5000 Loss 0.9135 Accuracy 0.5063\n",
      "Epoch 8 Batch 5050 Loss 0.9148 Accuracy 0.5061\n",
      "Epoch 8 Batch 5100 Loss 0.9162 Accuracy 0.5058\n",
      "Epoch 8 Batch 5150 Loss 0.9175 Accuracy 0.5056\n",
      "Epoch 8 Batch 5200 Loss 0.9187 Accuracy 0.5054\n",
      "Epoch 8 Batch 5250 Loss 0.9200 Accuracy 0.5051\n",
      "Epoch 8 Batch 5300 Loss 0.9212 Accuracy 0.5048\n",
      "Epoch 8 Batch 5350 Loss 0.9223 Accuracy 0.5045\n",
      "Epoch 8 Batch 5400 Loss 0.9236 Accuracy 0.5042\n",
      "Epoch 8 Batch 5450 Loss 0.9246 Accuracy 0.5039\n",
      "Epoch 8 Batch 5500 Loss 0.9256 Accuracy 0.5036\n",
      "Epoch 8 Batch 5550 Loss 0.9267 Accuracy 0.5034\n",
      "Epoch 8 Batch 5600 Loss 0.9276 Accuracy 0.5031\n",
      "Epoch 8 Batch 5650 Loss 0.9285 Accuracy 0.5029\n",
      "Epoch 8 Batch 5700 Loss 0.9294 Accuracy 0.5027\n",
      "Saving checkpoint for epoch 8 at ./TF/ckpt/ckpt-9\n",
      "Time taken for 1 epoch: 1465.193481206894 secs\n",
      "\n",
      "Start of epoch 9\n",
      "Epoch 9 Batch 0 Loss 0.9508 Accuracy 0.4630\n",
      "Epoch 9 Batch 50 Loss 1.0297 Accuracy 0.4864\n",
      "Epoch 9 Batch 100 Loss 1.0323 Accuracy 0.4863\n",
      "Epoch 9 Batch 150 Loss 1.0343 Accuracy 0.4864\n",
      "Epoch 9 Batch 200 Loss 1.0369 Accuracy 0.4867\n",
      "Epoch 9 Batch 250 Loss 1.0289 Accuracy 0.4871\n",
      "Epoch 9 Batch 300 Loss 1.0247 Accuracy 0.4873\n",
      "Epoch 9 Batch 350 Loss 1.0232 Accuracy 0.4877\n",
      "Epoch 9 Batch 400 Loss 1.0239 Accuracy 0.4867\n",
      "Epoch 9 Batch 450 Loss 1.0201 Accuracy 0.4861\n",
      "Epoch 9 Batch 500 Loss 1.0167 Accuracy 0.4857\n",
      "Epoch 9 Batch 550 Loss 1.0150 Accuracy 0.4855\n",
      "Epoch 9 Batch 600 Loss 1.0127 Accuracy 0.4857\n",
      "Epoch 9 Batch 650 Loss 1.0129 Accuracy 0.4859\n",
      "Epoch 9 Batch 700 Loss 1.0113 Accuracy 0.4861\n",
      "Epoch 9 Batch 750 Loss 1.0120 Accuracy 0.4865\n",
      "Epoch 9 Batch 800 Loss 1.0120 Accuracy 0.4870\n",
      "Epoch 9 Batch 850 Loss 1.0099 Accuracy 0.4872\n",
      "Epoch 9 Batch 900 Loss 1.0098 Accuracy 0.4871\n",
      "Epoch 9 Batch 950 Loss 1.0073 Accuracy 0.4872\n",
      "Epoch 9 Batch 1000 Loss 1.0054 Accuracy 0.4869\n",
      "Epoch 9 Batch 1050 Loss 1.0023 Accuracy 0.4871\n",
      "Epoch 9 Batch 1100 Loss 1.0008 Accuracy 0.4873\n",
      "Epoch 9 Batch 1150 Loss 0.9994 Accuracy 0.4874\n",
      "Epoch 9 Batch 1200 Loss 0.9985 Accuracy 0.4878\n",
      "Epoch 9 Batch 1250 Loss 0.9972 Accuracy 0.4881\n",
      "Epoch 9 Batch 1300 Loss 0.9954 Accuracy 0.4886\n",
      "Epoch 9 Batch 1350 Loss 0.9932 Accuracy 0.4893\n",
      "Epoch 9 Batch 1400 Loss 0.9911 Accuracy 0.4902\n",
      "Epoch 9 Batch 1450 Loss 0.9885 Accuracy 0.4910\n",
      "Epoch 9 Batch 1500 Loss 0.9863 Accuracy 0.4918\n",
      "Epoch 9 Batch 1550 Loss 0.9848 Accuracy 0.4925\n",
      "Epoch 9 Batch 1600 Loss 0.9827 Accuracy 0.4934\n",
      "Epoch 9 Batch 1650 Loss 0.9807 Accuracy 0.4942\n",
      "Epoch 9 Batch 1700 Loss 0.9778 Accuracy 0.4951\n",
      "Epoch 9 Batch 1750 Loss 0.9752 Accuracy 0.4958\n",
      "Epoch 9 Batch 1800 Loss 0.9726 Accuracy 0.4968\n",
      "Epoch 9 Batch 1850 Loss 0.9700 Accuracy 0.4976\n",
      "Epoch 9 Batch 1900 Loss 0.9681 Accuracy 0.4983\n",
      "Epoch 9 Batch 1950 Loss 0.9662 Accuracy 0.4991\n",
      "Epoch 9 Batch 2000 Loss 0.9645 Accuracy 0.4998\n",
      "Epoch 9 Batch 2050 Loss 0.9621 Accuracy 0.5002\n",
      "Epoch 9 Batch 2100 Loss 0.9598 Accuracy 0.5006\n",
      "Epoch 9 Batch 2150 Loss 0.9575 Accuracy 0.5007\n",
      "Epoch 9 Batch 2200 Loss 0.9546 Accuracy 0.5009\n",
      "Epoch 9 Batch 2250 Loss 0.9516 Accuracy 0.5009\n",
      "Epoch 9 Batch 2300 Loss 0.9488 Accuracy 0.5011\n",
      "Epoch 9 Batch 2350 Loss 0.9460 Accuracy 0.5014\n",
      "Epoch 9 Batch 2400 Loss 0.9441 Accuracy 0.5017\n",
      "Epoch 9 Batch 2450 Loss 0.9413 Accuracy 0.5020\n",
      "Epoch 9 Batch 2500 Loss 0.9381 Accuracy 0.5023\n",
      "Epoch 9 Batch 2550 Loss 0.9354 Accuracy 0.5024\n",
      "Epoch 9 Batch 2600 Loss 0.9329 Accuracy 0.5028\n",
      "Epoch 9 Batch 2650 Loss 0.9306 Accuracy 0.5031\n",
      "Epoch 9 Batch 2700 Loss 0.9281 Accuracy 0.5035\n",
      "Epoch 9 Batch 2750 Loss 0.9259 Accuracy 0.5039\n",
      "Epoch 9 Batch 2800 Loss 0.9239 Accuracy 0.5040\n",
      "Epoch 9 Batch 2850 Loss 0.9218 Accuracy 0.5044\n",
      "Epoch 9 Batch 2900 Loss 0.9195 Accuracy 0.5047\n",
      "Epoch 9 Batch 2950 Loss 0.9174 Accuracy 0.5050\n",
      "Epoch 9 Batch 3000 Loss 0.9153 Accuracy 0.5052\n",
      "Epoch 9 Batch 3050 Loss 0.9135 Accuracy 0.5054\n",
      "Epoch 9 Batch 3100 Loss 0.9120 Accuracy 0.5057\n",
      "Epoch 9 Batch 3150 Loss 0.9098 Accuracy 0.5059\n",
      "Epoch 9 Batch 3200 Loss 0.9083 Accuracy 0.5061\n",
      "Epoch 9 Batch 3250 Loss 0.9062 Accuracy 0.5063\n",
      "Epoch 9 Batch 3300 Loss 0.9042 Accuracy 0.5066\n",
      "Epoch 9 Batch 3350 Loss 0.9024 Accuracy 0.5069\n",
      "Epoch 9 Batch 3400 Loss 0.9003 Accuracy 0.5072\n",
      "Epoch 9 Batch 3450 Loss 0.8988 Accuracy 0.5074\n",
      "Epoch 9 Batch 3500 Loss 0.8971 Accuracy 0.5077\n",
      "Epoch 9 Batch 3550 Loss 0.8952 Accuracy 0.5080\n",
      "Epoch 9 Batch 3600 Loss 0.8934 Accuracy 0.5083\n",
      "Epoch 9 Batch 3650 Loss 0.8919 Accuracy 0.5085\n",
      "Epoch 9 Batch 3700 Loss 0.8903 Accuracy 0.5089\n",
      "Epoch 9 Batch 3750 Loss 0.8886 Accuracy 0.5093\n",
      "Epoch 9 Batch 3800 Loss 0.8870 Accuracy 0.5096\n",
      "Epoch 9 Batch 3850 Loss 0.8855 Accuracy 0.5099\n",
      "Epoch 9 Batch 3900 Loss 0.8843 Accuracy 0.5102\n",
      "Epoch 9 Batch 3950 Loss 0.8829 Accuracy 0.5105\n",
      "Epoch 9 Batch 4000 Loss 0.8816 Accuracy 0.5109\n",
      "Epoch 9 Batch 4050 Loss 0.8804 Accuracy 0.5112\n",
      "Epoch 9 Batch 4100 Loss 0.8794 Accuracy 0.5113\n",
      "Epoch 9 Batch 4150 Loss 0.8793 Accuracy 0.5114\n",
      "Epoch 9 Batch 4200 Loss 0.8797 Accuracy 0.5114\n",
      "Epoch 9 Batch 4250 Loss 0.8802 Accuracy 0.5113\n",
      "Epoch 9 Batch 4300 Loss 0.8811 Accuracy 0.5113\n",
      "Epoch 9 Batch 4350 Loss 0.8821 Accuracy 0.5112\n",
      "Epoch 9 Batch 4400 Loss 0.8833 Accuracy 0.5111\n",
      "Epoch 9 Batch 4450 Loss 0.8843 Accuracy 0.5108\n",
      "Epoch 9 Batch 4500 Loss 0.8856 Accuracy 0.5107\n",
      "Epoch 9 Batch 4550 Loss 0.8868 Accuracy 0.5104\n",
      "Epoch 9 Batch 4600 Loss 0.8883 Accuracy 0.5103\n",
      "Epoch 9 Batch 4650 Loss 0.8897 Accuracy 0.5101\n",
      "Epoch 9 Batch 4700 Loss 0.8912 Accuracy 0.5099\n",
      "Epoch 9 Batch 4750 Loss 0.8926 Accuracy 0.5097\n",
      "Epoch 9 Batch 4800 Loss 0.8938 Accuracy 0.5095\n",
      "Epoch 9 Batch 4850 Loss 0.8950 Accuracy 0.5093\n",
      "Epoch 9 Batch 4900 Loss 0.8960 Accuracy 0.5092\n",
      "Epoch 9 Batch 4950 Loss 0.8974 Accuracy 0.5090\n",
      "Epoch 9 Batch 5000 Loss 0.8986 Accuracy 0.5087\n",
      "Epoch 9 Batch 5050 Loss 0.9000 Accuracy 0.5085\n",
      "Epoch 9 Batch 5100 Loss 0.9012 Accuracy 0.5083\n",
      "Epoch 9 Batch 5150 Loss 0.9027 Accuracy 0.5080\n",
      "Epoch 9 Batch 5200 Loss 0.9041 Accuracy 0.5077\n",
      "Epoch 9 Batch 5250 Loss 0.9053 Accuracy 0.5075\n",
      "Epoch 9 Batch 5300 Loss 0.9064 Accuracy 0.5071\n",
      "Epoch 9 Batch 5350 Loss 0.9074 Accuracy 0.5068\n",
      "Epoch 9 Batch 5400 Loss 0.9085 Accuracy 0.5065\n",
      "Epoch 9 Batch 5450 Loss 0.9094 Accuracy 0.5063\n",
      "Epoch 9 Batch 5500 Loss 0.9105 Accuracy 0.5060\n",
      "Epoch 9 Batch 5550 Loss 0.9116 Accuracy 0.5057\n",
      "Epoch 9 Batch 5600 Loss 0.9125 Accuracy 0.5055\n",
      "Epoch 9 Batch 5650 Loss 0.9134 Accuracy 0.5052\n",
      "Epoch 9 Batch 5700 Loss 0.9142 Accuracy 0.5050\n",
      "Saving checkpoint for epoch 9 at ./TF/ckpt/ckpt-10\n",
      "Time taken for 1 epoch: 1464.2543091773987 secs\n",
      "\n",
      "Start of epoch 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Batch 0 Loss 0.8046 Accuracy 0.4794\n",
      "Epoch 10 Batch 50 Loss 1.0251 Accuracy 0.4874\n",
      "Epoch 10 Batch 100 Loss 1.0280 Accuracy 0.4860\n",
      "Epoch 10 Batch 150 Loss 1.0216 Accuracy 0.4856\n",
      "Epoch 10 Batch 200 Loss 1.0165 Accuracy 0.4861\n",
      "Epoch 10 Batch 250 Loss 1.0146 Accuracy 0.4873\n",
      "Epoch 10 Batch 300 Loss 1.0117 Accuracy 0.4875\n",
      "Epoch 10 Batch 350 Loss 1.0086 Accuracy 0.4879\n",
      "Epoch 10 Batch 400 Loss 1.0082 Accuracy 0.4881\n",
      "Epoch 10 Batch 450 Loss 1.0039 Accuracy 0.4882\n",
      "Epoch 10 Batch 500 Loss 1.0049 Accuracy 0.4883\n",
      "Epoch 10 Batch 550 Loss 1.0031 Accuracy 0.4877\n",
      "Epoch 10 Batch 600 Loss 1.0016 Accuracy 0.4880\n",
      "Epoch 10 Batch 650 Loss 1.0002 Accuracy 0.4884\n",
      "Epoch 10 Batch 700 Loss 0.9987 Accuracy 0.4890\n",
      "Epoch 10 Batch 750 Loss 0.9976 Accuracy 0.4892\n",
      "Epoch 10 Batch 800 Loss 0.9967 Accuracy 0.4898\n",
      "Epoch 10 Batch 850 Loss 0.9956 Accuracy 0.4898\n",
      "Epoch 10 Batch 900 Loss 0.9946 Accuracy 0.4898\n",
      "Epoch 10 Batch 950 Loss 0.9925 Accuracy 0.4901\n",
      "Epoch 10 Batch 1000 Loss 0.9906 Accuracy 0.4902\n",
      "Epoch 10 Batch 1050 Loss 0.9893 Accuracy 0.4903\n",
      "Epoch 10 Batch 1100 Loss 0.9878 Accuracy 0.4904\n",
      "Epoch 10 Batch 1150 Loss 0.9873 Accuracy 0.4907\n",
      "Epoch 10 Batch 1200 Loss 0.9864 Accuracy 0.4908\n",
      "Epoch 10 Batch 1250 Loss 0.9855 Accuracy 0.4910\n",
      "Epoch 10 Batch 1300 Loss 0.9835 Accuracy 0.4915\n",
      "Epoch 10 Batch 1350 Loss 0.9814 Accuracy 0.4918\n",
      "Epoch 10 Batch 1400 Loss 0.9787 Accuracy 0.4923\n",
      "Epoch 10 Batch 1450 Loss 0.9759 Accuracy 0.4930\n",
      "Epoch 10 Batch 1500 Loss 0.9736 Accuracy 0.4938\n",
      "Epoch 10 Batch 1550 Loss 0.9706 Accuracy 0.4948\n",
      "Epoch 10 Batch 1600 Loss 0.9682 Accuracy 0.4955\n",
      "Epoch 10 Batch 1650 Loss 0.9657 Accuracy 0.4963\n",
      "Epoch 10 Batch 1700 Loss 0.9636 Accuracy 0.4973\n",
      "Epoch 10 Batch 1750 Loss 0.9613 Accuracy 0.4981\n",
      "Epoch 10 Batch 1800 Loss 0.9596 Accuracy 0.4990\n",
      "Epoch 10 Batch 1850 Loss 0.9576 Accuracy 0.4997\n",
      "Epoch 10 Batch 1900 Loss 0.9548 Accuracy 0.5005\n",
      "Epoch 10 Batch 1950 Loss 0.9527 Accuracy 0.5012\n",
      "Epoch 10 Batch 2000 Loss 0.9504 Accuracy 0.5016\n",
      "Epoch 10 Batch 2050 Loss 0.9486 Accuracy 0.5022\n",
      "Epoch 10 Batch 2100 Loss 0.9460 Accuracy 0.5024\n",
      "Epoch 10 Batch 2150 Loss 0.9433 Accuracy 0.5028\n",
      "Epoch 10 Batch 2200 Loss 0.9411 Accuracy 0.5029\n",
      "Epoch 10 Batch 2250 Loss 0.9384 Accuracy 0.5032\n",
      "Epoch 10 Batch 2300 Loss 0.9354 Accuracy 0.5034\n",
      "Epoch 10 Batch 2350 Loss 0.9331 Accuracy 0.5036\n",
      "Epoch 10 Batch 2400 Loss 0.9305 Accuracy 0.5039\n",
      "Epoch 10 Batch 2450 Loss 0.9277 Accuracy 0.5042\n",
      "Epoch 10 Batch 2500 Loss 0.9249 Accuracy 0.5044\n",
      "Epoch 10 Batch 2550 Loss 0.9222 Accuracy 0.5047\n",
      "Epoch 10 Batch 2600 Loss 0.9193 Accuracy 0.5050\n",
      "Epoch 10 Batch 2650 Loss 0.9165 Accuracy 0.5054\n",
      "Epoch 10 Batch 2700 Loss 0.9140 Accuracy 0.5056\n",
      "Epoch 10 Batch 2750 Loss 0.9122 Accuracy 0.5060\n",
      "Epoch 10 Batch 2800 Loss 0.9097 Accuracy 0.5062\n",
      "Epoch 10 Batch 2850 Loss 0.9076 Accuracy 0.5065\n",
      "Epoch 10 Batch 2900 Loss 0.9060 Accuracy 0.5068\n",
      "Epoch 10 Batch 2950 Loss 0.9042 Accuracy 0.5071\n",
      "Epoch 10 Batch 3000 Loss 0.9025 Accuracy 0.5074\n",
      "Epoch 10 Batch 3050 Loss 0.9009 Accuracy 0.5076\n",
      "Epoch 10 Batch 3100 Loss 0.8988 Accuracy 0.5079\n",
      "Epoch 10 Batch 3150 Loss 0.8974 Accuracy 0.5081\n",
      "Epoch 10 Batch 3200 Loss 0.8955 Accuracy 0.5084\n",
      "Epoch 10 Batch 3250 Loss 0.8937 Accuracy 0.5086\n",
      "Epoch 10 Batch 3300 Loss 0.8920 Accuracy 0.5089\n",
      "Epoch 10 Batch 3350 Loss 0.8901 Accuracy 0.5091\n",
      "Epoch 10 Batch 3400 Loss 0.8881 Accuracy 0.5094\n",
      "Epoch 10 Batch 3450 Loss 0.8864 Accuracy 0.5097\n",
      "Epoch 10 Batch 3500 Loss 0.8846 Accuracy 0.5099\n",
      "Epoch 10 Batch 3550 Loss 0.8830 Accuracy 0.5102\n",
      "Epoch 10 Batch 3600 Loss 0.8814 Accuracy 0.5105\n",
      "Epoch 10 Batch 3650 Loss 0.8796 Accuracy 0.5107\n",
      "Epoch 10 Batch 3700 Loss 0.8777 Accuracy 0.5110\n",
      "Epoch 10 Batch 3750 Loss 0.8763 Accuracy 0.5114\n",
      "Epoch 10 Batch 3800 Loss 0.8748 Accuracy 0.5118\n",
      "Epoch 10 Batch 3850 Loss 0.8734 Accuracy 0.5120\n",
      "Epoch 10 Batch 3900 Loss 0.8724 Accuracy 0.5123\n",
      "Epoch 10 Batch 3950 Loss 0.8711 Accuracy 0.5126\n",
      "Epoch 10 Batch 4000 Loss 0.8701 Accuracy 0.5129\n",
      "Epoch 10 Batch 4050 Loss 0.8690 Accuracy 0.5132\n",
      "Epoch 10 Batch 4100 Loss 0.8680 Accuracy 0.5134\n",
      "Epoch 10 Batch 4150 Loss 0.8676 Accuracy 0.5135\n",
      "Epoch 10 Batch 4200 Loss 0.8677 Accuracy 0.5134\n",
      "Epoch 10 Batch 4250 Loss 0.8682 Accuracy 0.5135\n",
      "Epoch 10 Batch 4300 Loss 0.8690 Accuracy 0.5134\n",
      "Epoch 10 Batch 4350 Loss 0.8701 Accuracy 0.5133\n",
      "Epoch 10 Batch 4400 Loss 0.8712 Accuracy 0.5131\n",
      "Epoch 10 Batch 4450 Loss 0.8723 Accuracy 0.5129\n",
      "Epoch 10 Batch 4500 Loss 0.8734 Accuracy 0.5128\n",
      "Epoch 10 Batch 4550 Loss 0.8749 Accuracy 0.5125\n",
      "Epoch 10 Batch 4600 Loss 0.8762 Accuracy 0.5123\n",
      "Epoch 10 Batch 4650 Loss 0.8775 Accuracy 0.5121\n",
      "Epoch 10 Batch 4700 Loss 0.8788 Accuracy 0.5120\n",
      "Epoch 10 Batch 4750 Loss 0.8798 Accuracy 0.5118\n",
      "Epoch 10 Batch 4800 Loss 0.8811 Accuracy 0.5116\n",
      "Epoch 10 Batch 4850 Loss 0.8827 Accuracy 0.5114\n",
      "Epoch 10 Batch 4900 Loss 0.8838 Accuracy 0.5112\n",
      "Epoch 10 Batch 4950 Loss 0.8852 Accuracy 0.5110\n",
      "Epoch 10 Batch 5000 Loss 0.8867 Accuracy 0.5108\n",
      "Epoch 10 Batch 5050 Loss 0.8881 Accuracy 0.5106\n",
      "Epoch 10 Batch 5100 Loss 0.8893 Accuracy 0.5104\n",
      "Epoch 10 Batch 5150 Loss 0.8905 Accuracy 0.5101\n",
      "Epoch 10 Batch 5200 Loss 0.8918 Accuracy 0.5098\n",
      "Epoch 10 Batch 5250 Loss 0.8930 Accuracy 0.5095\n",
      "Epoch 10 Batch 5300 Loss 0.8940 Accuracy 0.5092\n",
      "Epoch 10 Batch 5350 Loss 0.8951 Accuracy 0.5089\n",
      "Epoch 10 Batch 5400 Loss 0.8961 Accuracy 0.5087\n",
      "Epoch 10 Batch 5450 Loss 0.8972 Accuracy 0.5084\n",
      "Epoch 10 Batch 5500 Loss 0.8983 Accuracy 0.5082\n",
      "Epoch 10 Batch 5550 Loss 0.8996 Accuracy 0.5079\n",
      "Epoch 10 Batch 5600 Loss 0.9004 Accuracy 0.5076\n",
      "Epoch 10 Batch 5650 Loss 0.9014 Accuracy 0.5074\n",
      "Epoch 10 Batch 5700 Loss 0.9025 Accuracy 0.5071\n",
      "Saving checkpoint for epoch 10 at ./TF/ckpt/ckpt-11\n",
      "Time taken for 1 epoch: 1465.7571549415588 secs\n",
      "\n",
      "Start of epoch 11\n",
      "Epoch 11 Batch 0 Loss 0.9388 Accuracy 0.5008\n",
      "Epoch 11 Batch 50 Loss 1.0069 Accuracy 0.4930\n",
      "Epoch 11 Batch 100 Loss 0.9978 Accuracy 0.4922\n",
      "Epoch 11 Batch 150 Loss 1.0034 Accuracy 0.4910\n",
      "Epoch 11 Batch 200 Loss 0.9981 Accuracy 0.4906\n",
      "Epoch 11 Batch 250 Loss 0.9991 Accuracy 0.4907\n",
      "Epoch 11 Batch 300 Loss 1.0010 Accuracy 0.4900\n",
      "Epoch 11 Batch 350 Loss 1.0004 Accuracy 0.4904\n",
      "Epoch 11 Batch 400 Loss 0.9981 Accuracy 0.4908\n",
      "Epoch 11 Batch 450 Loss 0.9959 Accuracy 0.4906\n",
      "Epoch 11 Batch 500 Loss 0.9916 Accuracy 0.4907\n",
      "Epoch 11 Batch 550 Loss 0.9918 Accuracy 0.4909\n",
      "Epoch 11 Batch 600 Loss 0.9906 Accuracy 0.4909\n",
      "Epoch 11 Batch 650 Loss 0.9910 Accuracy 0.4909\n",
      "Epoch 11 Batch 700 Loss 0.9887 Accuracy 0.4911\n",
      "Epoch 11 Batch 750 Loss 0.9868 Accuracy 0.4915\n",
      "Epoch 11 Batch 800 Loss 0.9878 Accuracy 0.4914\n",
      "Epoch 11 Batch 850 Loss 0.9867 Accuracy 0.4917\n",
      "Epoch 11 Batch 900 Loss 0.9851 Accuracy 0.4917\n",
      "Epoch 11 Batch 950 Loss 0.9837 Accuracy 0.4914\n",
      "Epoch 11 Batch 1000 Loss 0.9810 Accuracy 0.4915\n",
      "Epoch 11 Batch 1050 Loss 0.9794 Accuracy 0.4916\n",
      "Epoch 11 Batch 1100 Loss 0.9779 Accuracy 0.4919\n",
      "Epoch 11 Batch 1150 Loss 0.9765 Accuracy 0.4922\n",
      "Epoch 11 Batch 1200 Loss 0.9751 Accuracy 0.4926\n",
      "Epoch 11 Batch 1250 Loss 0.9732 Accuracy 0.4929\n",
      "Epoch 11 Batch 1300 Loss 0.9716 Accuracy 0.4933\n",
      "Epoch 11 Batch 1350 Loss 0.9696 Accuracy 0.4941\n",
      "Epoch 11 Batch 1400 Loss 0.9676 Accuracy 0.4948\n",
      "Epoch 11 Batch 1450 Loss 0.9653 Accuracy 0.4953\n",
      "Epoch 11 Batch 1500 Loss 0.9631 Accuracy 0.4962\n",
      "Epoch 11 Batch 1550 Loss 0.9610 Accuracy 0.4969\n",
      "Epoch 11 Batch 1600 Loss 0.9585 Accuracy 0.4977\n",
      "Epoch 11 Batch 1650 Loss 0.9552 Accuracy 0.4985\n",
      "Epoch 11 Batch 1700 Loss 0.9525 Accuracy 0.4995\n",
      "Epoch 11 Batch 1750 Loss 0.9506 Accuracy 0.5002\n",
      "Epoch 11 Batch 1800 Loss 0.9485 Accuracy 0.5012\n",
      "Epoch 11 Batch 1850 Loss 0.9463 Accuracy 0.5019\n",
      "Epoch 11 Batch 1900 Loss 0.9442 Accuracy 0.5026\n",
      "Epoch 11 Batch 1950 Loss 0.9417 Accuracy 0.5035\n",
      "Epoch 11 Batch 2000 Loss 0.9395 Accuracy 0.5040\n",
      "Epoch 11 Batch 2050 Loss 0.9368 Accuracy 0.5045\n",
      "Epoch 11 Batch 2100 Loss 0.9345 Accuracy 0.5049\n",
      "Epoch 11 Batch 2150 Loss 0.9317 Accuracy 0.5051\n",
      "Epoch 11 Batch 2200 Loss 0.9290 Accuracy 0.5053\n",
      "Epoch 11 Batch 2250 Loss 0.9262 Accuracy 0.5054\n",
      "Epoch 11 Batch 2300 Loss 0.9236 Accuracy 0.5057\n",
      "Epoch 11 Batch 2350 Loss 0.9209 Accuracy 0.5059\n",
      "Epoch 11 Batch 2400 Loss 0.9182 Accuracy 0.5060\n",
      "Epoch 11 Batch 2450 Loss 0.9156 Accuracy 0.5063\n",
      "Epoch 11 Batch 2500 Loss 0.9128 Accuracy 0.5065\n",
      "Epoch 11 Batch 2550 Loss 0.9101 Accuracy 0.5068\n",
      "Epoch 11 Batch 2600 Loss 0.9078 Accuracy 0.5072\n",
      "Epoch 11 Batch 2650 Loss 0.9053 Accuracy 0.5074\n",
      "Epoch 11 Batch 2700 Loss 0.9026 Accuracy 0.5077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11 Batch 2750 Loss 0.9009 Accuracy 0.5080\n",
      "Epoch 11 Batch 2800 Loss 0.8989 Accuracy 0.5082\n",
      "Epoch 11 Batch 2850 Loss 0.8971 Accuracy 0.5085\n",
      "Epoch 11 Batch 2900 Loss 0.8951 Accuracy 0.5087\n",
      "Epoch 11 Batch 2950 Loss 0.8928 Accuracy 0.5089\n",
      "Epoch 11 Batch 3000 Loss 0.8909 Accuracy 0.5092\n",
      "Epoch 11 Batch 3050 Loss 0.8894 Accuracy 0.5095\n",
      "Epoch 11 Batch 3100 Loss 0.8877 Accuracy 0.5099\n",
      "Epoch 11 Batch 3150 Loss 0.8860 Accuracy 0.5101\n",
      "Epoch 11 Batch 3200 Loss 0.8842 Accuracy 0.5104\n",
      "Epoch 11 Batch 3250 Loss 0.8823 Accuracy 0.5105\n",
      "Epoch 11 Batch 3300 Loss 0.8804 Accuracy 0.5109\n",
      "Epoch 11 Batch 3350 Loss 0.8784 Accuracy 0.5112\n",
      "Epoch 11 Batch 3400 Loss 0.8766 Accuracy 0.5115\n",
      "Epoch 11 Batch 3450 Loss 0.8751 Accuracy 0.5117\n",
      "Epoch 11 Batch 3500 Loss 0.8734 Accuracy 0.5120\n",
      "Epoch 11 Batch 3550 Loss 0.8713 Accuracy 0.5124\n",
      "Epoch 11 Batch 3600 Loss 0.8695 Accuracy 0.5126\n",
      "Epoch 11 Batch 3650 Loss 0.8677 Accuracy 0.5129\n",
      "Epoch 11 Batch 3700 Loss 0.8661 Accuracy 0.5132\n",
      "Epoch 11 Batch 3750 Loss 0.8643 Accuracy 0.5134\n",
      "Epoch 11 Batch 3800 Loss 0.8628 Accuracy 0.5138\n",
      "Epoch 11 Batch 3850 Loss 0.8615 Accuracy 0.5141\n",
      "Epoch 11 Batch 3900 Loss 0.8602 Accuracy 0.5144\n",
      "Epoch 11 Batch 3950 Loss 0.8590 Accuracy 0.5149\n",
      "Epoch 11 Batch 4000 Loss 0.8578 Accuracy 0.5151\n",
      "Epoch 11 Batch 4050 Loss 0.8566 Accuracy 0.5155\n",
      "Epoch 11 Batch 4100 Loss 0.8557 Accuracy 0.5157\n",
      "Epoch 11 Batch 4150 Loss 0.8553 Accuracy 0.5158\n",
      "Epoch 11 Batch 4200 Loss 0.8557 Accuracy 0.5157\n",
      "Epoch 11 Batch 4250 Loss 0.8561 Accuracy 0.5156\n",
      "Epoch 11 Batch 4300 Loss 0.8571 Accuracy 0.5156\n",
      "Epoch 11 Batch 4350 Loss 0.8582 Accuracy 0.5154\n",
      "Epoch 11 Batch 4400 Loss 0.8590 Accuracy 0.5153\n",
      "Epoch 11 Batch 4450 Loss 0.8601 Accuracy 0.5150\n",
      "Epoch 11 Batch 4500 Loss 0.8613 Accuracy 0.5148\n",
      "Epoch 11 Batch 4550 Loss 0.8627 Accuracy 0.5146\n",
      "Epoch 11 Batch 4600 Loss 0.8644 Accuracy 0.5144\n",
      "Epoch 11 Batch 4650 Loss 0.8656 Accuracy 0.5142\n",
      "Epoch 11 Batch 4700 Loss 0.8668 Accuracy 0.5139\n",
      "Epoch 11 Batch 4750 Loss 0.8684 Accuracy 0.5138\n",
      "Epoch 11 Batch 4800 Loss 0.8696 Accuracy 0.5136\n",
      "Epoch 11 Batch 4850 Loss 0.8709 Accuracy 0.5134\n",
      "Epoch 11 Batch 4900 Loss 0.8721 Accuracy 0.5132\n",
      "Epoch 11 Batch 4950 Loss 0.8733 Accuracy 0.5130\n",
      "Epoch 11 Batch 5000 Loss 0.8748 Accuracy 0.5128\n",
      "Epoch 11 Batch 5050 Loss 0.8762 Accuracy 0.5126\n",
      "Epoch 11 Batch 5100 Loss 0.8776 Accuracy 0.5123\n",
      "Epoch 11 Batch 5150 Loss 0.8790 Accuracy 0.5120\n",
      "Epoch 11 Batch 5200 Loss 0.8803 Accuracy 0.5117\n",
      "Epoch 11 Batch 5250 Loss 0.8814 Accuracy 0.5114\n",
      "Epoch 11 Batch 5300 Loss 0.8825 Accuracy 0.5112\n",
      "Epoch 11 Batch 5350 Loss 0.8834 Accuracy 0.5109\n",
      "Epoch 11 Batch 5400 Loss 0.8846 Accuracy 0.5106\n",
      "Epoch 11 Batch 5450 Loss 0.8857 Accuracy 0.5103\n",
      "Epoch 11 Batch 5500 Loss 0.8869 Accuracy 0.5101\n",
      "Epoch 11 Batch 5550 Loss 0.8879 Accuracy 0.5098\n",
      "Epoch 11 Batch 5600 Loss 0.8889 Accuracy 0.5096\n",
      "Epoch 11 Batch 5650 Loss 0.8900 Accuracy 0.5093\n",
      "Epoch 11 Batch 5700 Loss 0.8910 Accuracy 0.5091\n",
      "Saving checkpoint for epoch 11 at ./TF/ckpt/ckpt-12\n",
      "Time taken for 1 epoch: 1466.491338968277 secs\n",
      "\n",
      "Start of epoch 12\n",
      "Epoch 12 Batch 0 Loss 0.9055 Accuracy 0.4893\n",
      "Epoch 12 Batch 50 Loss 1.0007 Accuracy 0.4923\n",
      "Epoch 12 Batch 100 Loss 0.9985 Accuracy 0.4944\n",
      "Epoch 12 Batch 150 Loss 0.9993 Accuracy 0.4920\n",
      "Epoch 12 Batch 200 Loss 0.9968 Accuracy 0.4912\n",
      "Epoch 12 Batch 250 Loss 0.9917 Accuracy 0.4912\n",
      "Epoch 12 Batch 300 Loss 0.9894 Accuracy 0.4910\n",
      "Epoch 12 Batch 350 Loss 0.9882 Accuracy 0.4914\n",
      "Epoch 12 Batch 400 Loss 0.9839 Accuracy 0.4925\n",
      "Epoch 12 Batch 450 Loss 0.9807 Accuracy 0.4922\n",
      "Epoch 12 Batch 500 Loss 0.9806 Accuracy 0.4922\n",
      "Epoch 12 Batch 550 Loss 0.9786 Accuracy 0.4923\n",
      "Epoch 12 Batch 600 Loss 0.9786 Accuracy 0.4925\n",
      "Epoch 12 Batch 650 Loss 0.9760 Accuracy 0.4931\n",
      "Epoch 12 Batch 700 Loss 0.9744 Accuracy 0.4938\n",
      "Epoch 12 Batch 750 Loss 0.9749 Accuracy 0.4940\n",
      "Epoch 12 Batch 800 Loss 0.9741 Accuracy 0.4936\n",
      "Epoch 12 Batch 850 Loss 0.9730 Accuracy 0.4936\n",
      "Epoch 12 Batch 900 Loss 0.9713 Accuracy 0.4935\n",
      "Epoch 12 Batch 950 Loss 0.9700 Accuracy 0.4934\n",
      "Epoch 12 Batch 1000 Loss 0.9670 Accuracy 0.4936\n",
      "Epoch 12 Batch 1050 Loss 0.9671 Accuracy 0.4938\n",
      "Epoch 12 Batch 1100 Loss 0.9668 Accuracy 0.4940\n",
      "Epoch 12 Batch 1150 Loss 0.9654 Accuracy 0.4942\n",
      "Epoch 12 Batch 1200 Loss 0.9641 Accuracy 0.4944\n",
      "Epoch 12 Batch 1250 Loss 0.9622 Accuracy 0.4946\n",
      "Epoch 12 Batch 1300 Loss 0.9603 Accuracy 0.4949\n",
      "Epoch 12 Batch 1350 Loss 0.9590 Accuracy 0.4956\n",
      "Epoch 12 Batch 1400 Loss 0.9570 Accuracy 0.4965\n",
      "Epoch 12 Batch 1450 Loss 0.9544 Accuracy 0.4972\n",
      "Epoch 12 Batch 1500 Loss 0.9516 Accuracy 0.4980\n",
      "Epoch 12 Batch 1550 Loss 0.9491 Accuracy 0.4989\n",
      "Epoch 12 Batch 1600 Loss 0.9462 Accuracy 0.4998\n",
      "Epoch 12 Batch 1650 Loss 0.9439 Accuracy 0.5006\n",
      "Epoch 12 Batch 1700 Loss 0.9410 Accuracy 0.5014\n",
      "Epoch 12 Batch 1750 Loss 0.9391 Accuracy 0.5021\n",
      "Epoch 12 Batch 1800 Loss 0.9374 Accuracy 0.5028\n",
      "Epoch 12 Batch 1850 Loss 0.9348 Accuracy 0.5036\n",
      "Epoch 12 Batch 1900 Loss 0.9325 Accuracy 0.5044\n",
      "Epoch 12 Batch 1950 Loss 0.9309 Accuracy 0.5052\n",
      "Epoch 12 Batch 2000 Loss 0.9289 Accuracy 0.5057\n",
      "Epoch 12 Batch 2050 Loss 0.9269 Accuracy 0.5061\n",
      "Epoch 12 Batch 2100 Loss 0.9242 Accuracy 0.5066\n",
      "Epoch 12 Batch 2150 Loss 0.9215 Accuracy 0.5068\n",
      "Epoch 12 Batch 2200 Loss 0.9185 Accuracy 0.5069\n",
      "Epoch 12 Batch 2250 Loss 0.9155 Accuracy 0.5072\n",
      "Epoch 12 Batch 2300 Loss 0.9129 Accuracy 0.5074\n",
      "Epoch 12 Batch 2350 Loss 0.9103 Accuracy 0.5075\n",
      "Epoch 12 Batch 2400 Loss 0.9073 Accuracy 0.5077\n",
      "Epoch 12 Batch 2450 Loss 0.9046 Accuracy 0.5080\n",
      "Epoch 12 Batch 2500 Loss 0.9021 Accuracy 0.5083\n",
      "Epoch 12 Batch 2550 Loss 0.8995 Accuracy 0.5086\n",
      "Epoch 12 Batch 2600 Loss 0.8968 Accuracy 0.5089\n",
      "Epoch 12 Batch 2650 Loss 0.8941 Accuracy 0.5093\n",
      "Epoch 12 Batch 2700 Loss 0.8914 Accuracy 0.5096\n",
      "Epoch 12 Batch 2750 Loss 0.8893 Accuracy 0.5098\n",
      "Epoch 12 Batch 2800 Loss 0.8869 Accuracy 0.5100\n",
      "Epoch 12 Batch 2850 Loss 0.8851 Accuracy 0.5103\n",
      "Epoch 12 Batch 2900 Loss 0.8834 Accuracy 0.5106\n",
      "Epoch 12 Batch 2950 Loss 0.8816 Accuracy 0.5107\n",
      "Epoch 12 Batch 3000 Loss 0.8798 Accuracy 0.5110\n",
      "Epoch 12 Batch 3050 Loss 0.8784 Accuracy 0.5112\n",
      "Epoch 12 Batch 3100 Loss 0.8764 Accuracy 0.5115\n",
      "Epoch 12 Batch 3150 Loss 0.8747 Accuracy 0.5117\n",
      "Epoch 12 Batch 3200 Loss 0.8729 Accuracy 0.5117\n",
      "Epoch 12 Batch 3250 Loss 0.8714 Accuracy 0.5120\n",
      "Epoch 12 Batch 3300 Loss 0.8695 Accuracy 0.5122\n",
      "Epoch 12 Batch 3350 Loss 0.8678 Accuracy 0.5125\n",
      "Epoch 12 Batch 3400 Loss 0.8659 Accuracy 0.5129\n",
      "Epoch 12 Batch 3450 Loss 0.8643 Accuracy 0.5132\n",
      "Epoch 12 Batch 3500 Loss 0.8629 Accuracy 0.5135\n",
      "Epoch 12 Batch 3550 Loss 0.8612 Accuracy 0.5138\n",
      "Epoch 12 Batch 3600 Loss 0.8595 Accuracy 0.5142\n",
      "Epoch 12 Batch 3650 Loss 0.8580 Accuracy 0.5145\n",
      "Epoch 12 Batch 3700 Loss 0.8563 Accuracy 0.5148\n",
      "Epoch 12 Batch 3750 Loss 0.8548 Accuracy 0.5152\n",
      "Epoch 12 Batch 3800 Loss 0.8536 Accuracy 0.5154\n",
      "Epoch 12 Batch 3850 Loss 0.8523 Accuracy 0.5156\n",
      "Epoch 12 Batch 3900 Loss 0.8510 Accuracy 0.5159\n",
      "Epoch 12 Batch 3950 Loss 0.8497 Accuracy 0.5162\n",
      "Epoch 12 Batch 4000 Loss 0.8486 Accuracy 0.5165\n",
      "Epoch 12 Batch 4050 Loss 0.8474 Accuracy 0.5167\n",
      "Epoch 12 Batch 4100 Loss 0.8464 Accuracy 0.5169\n",
      "Epoch 12 Batch 4150 Loss 0.8462 Accuracy 0.5171\n",
      "Epoch 12 Batch 4200 Loss 0.8464 Accuracy 0.5172\n",
      "Epoch 12 Batch 4250 Loss 0.8468 Accuracy 0.5171\n",
      "Epoch 12 Batch 4300 Loss 0.8477 Accuracy 0.5170\n",
      "Epoch 12 Batch 4350 Loss 0.8486 Accuracy 0.5168\n",
      "Epoch 12 Batch 4400 Loss 0.8499 Accuracy 0.5167\n",
      "Epoch 12 Batch 4450 Loss 0.8509 Accuracy 0.5165\n",
      "Epoch 12 Batch 4500 Loss 0.8523 Accuracy 0.5163\n",
      "Epoch 12 Batch 4550 Loss 0.8536 Accuracy 0.5161\n",
      "Epoch 12 Batch 4600 Loss 0.8547 Accuracy 0.5159\n",
      "Epoch 12 Batch 4650 Loss 0.8561 Accuracy 0.5157\n",
      "Epoch 12 Batch 4700 Loss 0.8576 Accuracy 0.5155\n",
      "Epoch 12 Batch 4750 Loss 0.8591 Accuracy 0.5153\n",
      "Epoch 12 Batch 4800 Loss 0.8604 Accuracy 0.5151\n",
      "Epoch 12 Batch 4850 Loss 0.8617 Accuracy 0.5149\n",
      "Epoch 12 Batch 4900 Loss 0.8629 Accuracy 0.5147\n",
      "Epoch 12 Batch 4950 Loss 0.8641 Accuracy 0.5145\n",
      "Epoch 12 Batch 5000 Loss 0.8652 Accuracy 0.5143\n",
      "Epoch 12 Batch 5050 Loss 0.8664 Accuracy 0.5140\n",
      "Epoch 12 Batch 5100 Loss 0.8678 Accuracy 0.5138\n",
      "Epoch 12 Batch 5150 Loss 0.8690 Accuracy 0.5136\n",
      "Epoch 12 Batch 5200 Loss 0.8703 Accuracy 0.5133\n",
      "Epoch 12 Batch 5250 Loss 0.8716 Accuracy 0.5130\n",
      "Epoch 12 Batch 5300 Loss 0.8728 Accuracy 0.5127\n",
      "Epoch 12 Batch 5350 Loss 0.8741 Accuracy 0.5125\n",
      "Epoch 12 Batch 5400 Loss 0.8752 Accuracy 0.5121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Batch 5450 Loss 0.8764 Accuracy 0.5119\n",
      "Epoch 12 Batch 5500 Loss 0.8774 Accuracy 0.5116\n",
      "Epoch 12 Batch 5550 Loss 0.8783 Accuracy 0.5114\n",
      "Epoch 12 Batch 5600 Loss 0.8792 Accuracy 0.5111\n",
      "Epoch 12 Batch 5650 Loss 0.8803 Accuracy 0.5109\n",
      "Epoch 12 Batch 5700 Loss 0.8813 Accuracy 0.5106\n",
      "Saving checkpoint for epoch 12 at ./TF/ckpt/ckpt-13\n",
      "Time taken for 1 epoch: 1469.0225212574005 secs\n",
      "\n",
      "Start of epoch 13\n",
      "Epoch 13 Batch 0 Loss 1.0444 Accuracy 0.4803\n",
      "Epoch 13 Batch 50 Loss 0.9822 Accuracy 0.4965\n",
      "Epoch 13 Batch 100 Loss 0.9788 Accuracy 0.4946\n",
      "Epoch 13 Batch 150 Loss 0.9852 Accuracy 0.4956\n",
      "Epoch 13 Batch 200 Loss 0.9883 Accuracy 0.4952\n",
      "Epoch 13 Batch 250 Loss 0.9868 Accuracy 0.4950\n",
      "Epoch 13 Batch 300 Loss 0.9848 Accuracy 0.4946\n",
      "Epoch 13 Batch 350 Loss 0.9801 Accuracy 0.4945\n",
      "Epoch 13 Batch 400 Loss 0.9778 Accuracy 0.4942\n",
      "Epoch 13 Batch 450 Loss 0.9764 Accuracy 0.4942\n",
      "Epoch 13 Batch 500 Loss 0.9756 Accuracy 0.4941\n",
      "Epoch 13 Batch 550 Loss 0.9721 Accuracy 0.4938\n",
      "Epoch 13 Batch 600 Loss 0.9711 Accuracy 0.4936\n",
      "Epoch 13 Batch 650 Loss 0.9712 Accuracy 0.4941\n",
      "Epoch 13 Batch 700 Loss 0.9714 Accuracy 0.4943\n",
      "Epoch 13 Batch 750 Loss 0.9712 Accuracy 0.4944\n",
      "Epoch 13 Batch 800 Loss 0.9706 Accuracy 0.4949\n",
      "Epoch 13 Batch 850 Loss 0.9691 Accuracy 0.4947\n",
      "Epoch 13 Batch 900 Loss 0.9674 Accuracy 0.4947\n",
      "Epoch 13 Batch 950 Loss 0.9656 Accuracy 0.4948\n",
      "Epoch 13 Batch 1000 Loss 0.9631 Accuracy 0.4949\n",
      "Epoch 13 Batch 1050 Loss 0.9607 Accuracy 0.4951\n",
      "Epoch 13 Batch 1100 Loss 0.9597 Accuracy 0.4951\n",
      "Epoch 13 Batch 1150 Loss 0.9596 Accuracy 0.4952\n",
      "Epoch 13 Batch 1200 Loss 0.9568 Accuracy 0.4955\n",
      "Epoch 13 Batch 1250 Loss 0.9554 Accuracy 0.4958\n",
      "Epoch 13 Batch 1300 Loss 0.9529 Accuracy 0.4964\n",
      "Epoch 13 Batch 1350 Loss 0.9507 Accuracy 0.4973\n",
      "Epoch 13 Batch 1400 Loss 0.9485 Accuracy 0.4980\n",
      "Epoch 13 Batch 1450 Loss 0.9457 Accuracy 0.4989\n",
      "Epoch 13 Batch 1500 Loss 0.9423 Accuracy 0.4997\n",
      "Epoch 13 Batch 1550 Loss 0.9395 Accuracy 0.5005\n",
      "Epoch 13 Batch 1600 Loss 0.9370 Accuracy 0.5014\n",
      "Epoch 13 Batch 1650 Loss 0.9352 Accuracy 0.5021\n",
      "Epoch 13 Batch 1700 Loss 0.9332 Accuracy 0.5030\n",
      "Epoch 13 Batch 1750 Loss 0.9307 Accuracy 0.5038\n",
      "Epoch 13 Batch 1800 Loss 0.9281 Accuracy 0.5045\n",
      "Epoch 13 Batch 1850 Loss 0.9257 Accuracy 0.5053\n",
      "Epoch 13 Batch 1900 Loss 0.9236 Accuracy 0.5061\n",
      "Epoch 13 Batch 1950 Loss 0.9223 Accuracy 0.5067\n",
      "Epoch 13 Batch 2000 Loss 0.9210 Accuracy 0.5073\n",
      "Epoch 13 Batch 2050 Loss 0.9190 Accuracy 0.5078\n",
      "Epoch 13 Batch 2100 Loss 0.9165 Accuracy 0.5081\n",
      "Epoch 13 Batch 2150 Loss 0.9141 Accuracy 0.5084\n",
      "Epoch 13 Batch 2200 Loss 0.9109 Accuracy 0.5085\n",
      "Epoch 13 Batch 2250 Loss 0.9077 Accuracy 0.5087\n",
      "Epoch 13 Batch 2300 Loss 0.9051 Accuracy 0.5088\n",
      "Epoch 13 Batch 2350 Loss 0.9025 Accuracy 0.5089\n",
      "Epoch 13 Batch 2400 Loss 0.8998 Accuracy 0.5092\n",
      "Epoch 13 Batch 2450 Loss 0.8972 Accuracy 0.5094\n",
      "Epoch 13 Batch 2500 Loss 0.8943 Accuracy 0.5097\n",
      "Epoch 13 Batch 2550 Loss 0.8917 Accuracy 0.5100\n",
      "Epoch 13 Batch 2600 Loss 0.8888 Accuracy 0.5104\n",
      "Epoch 13 Batch 2650 Loss 0.8857 Accuracy 0.5107\n",
      "Epoch 13 Batch 2700 Loss 0.8837 Accuracy 0.5111\n",
      "Epoch 13 Batch 2750 Loss 0.8814 Accuracy 0.5114\n",
      "Epoch 13 Batch 2800 Loss 0.8792 Accuracy 0.5116\n",
      "Epoch 13 Batch 2850 Loss 0.8773 Accuracy 0.5118\n",
      "Epoch 13 Batch 2900 Loss 0.8758 Accuracy 0.5120\n",
      "Epoch 13 Batch 2950 Loss 0.8739 Accuracy 0.5122\n",
      "Epoch 13 Batch 3000 Loss 0.8720 Accuracy 0.5125\n",
      "Epoch 13 Batch 3050 Loss 0.8703 Accuracy 0.5127\n",
      "Epoch 13 Batch 3100 Loss 0.8686 Accuracy 0.5130\n",
      "Epoch 13 Batch 3150 Loss 0.8668 Accuracy 0.5133\n",
      "Epoch 13 Batch 3200 Loss 0.8651 Accuracy 0.5135\n",
      "Epoch 13 Batch 3250 Loss 0.8630 Accuracy 0.5137\n",
      "Epoch 13 Batch 3300 Loss 0.8611 Accuracy 0.5139\n",
      "Epoch 13 Batch 3350 Loss 0.8593 Accuracy 0.5141\n",
      "Epoch 13 Batch 3400 Loss 0.8575 Accuracy 0.5145\n",
      "Epoch 13 Batch 3450 Loss 0.8557 Accuracy 0.5148\n",
      "Epoch 13 Batch 3500 Loss 0.8544 Accuracy 0.5150\n",
      "Epoch 13 Batch 3550 Loss 0.8528 Accuracy 0.5152\n",
      "Epoch 13 Batch 3600 Loss 0.8512 Accuracy 0.5156\n",
      "Epoch 13 Batch 3650 Loss 0.8494 Accuracy 0.5159\n",
      "Epoch 13 Batch 3700 Loss 0.8478 Accuracy 0.5163\n",
      "Epoch 13 Batch 3750 Loss 0.8463 Accuracy 0.5167\n",
      "Epoch 13 Batch 3800 Loss 0.8449 Accuracy 0.5169\n",
      "Epoch 13 Batch 3850 Loss 0.8435 Accuracy 0.5172\n",
      "Epoch 13 Batch 3900 Loss 0.8422 Accuracy 0.5174\n",
      "Epoch 13 Batch 3950 Loss 0.8409 Accuracy 0.5177\n",
      "Epoch 13 Batch 4000 Loss 0.8394 Accuracy 0.5180\n",
      "Epoch 13 Batch 4050 Loss 0.8382 Accuracy 0.5183\n",
      "Epoch 13 Batch 4100 Loss 0.8372 Accuracy 0.5185\n",
      "Epoch 13 Batch 4150 Loss 0.8369 Accuracy 0.5187\n",
      "Epoch 13 Batch 4200 Loss 0.8373 Accuracy 0.5187\n",
      "Epoch 13 Batch 4250 Loss 0.8379 Accuracy 0.5186\n",
      "Epoch 13 Batch 4300 Loss 0.8385 Accuracy 0.5185\n",
      "Epoch 13 Batch 4350 Loss 0.8396 Accuracy 0.5183\n",
      "Epoch 13 Batch 4400 Loss 0.8407 Accuracy 0.5181\n",
      "Epoch 13 Batch 4450 Loss 0.8416 Accuracy 0.5179\n",
      "Epoch 13 Batch 4500 Loss 0.8431 Accuracy 0.5177\n",
      "Epoch 13 Batch 4550 Loss 0.8444 Accuracy 0.5175\n",
      "Epoch 13 Batch 4600 Loss 0.8458 Accuracy 0.5174\n",
      "Epoch 13 Batch 4650 Loss 0.8472 Accuracy 0.5171\n",
      "Epoch 13 Batch 4700 Loss 0.8486 Accuracy 0.5170\n",
      "Epoch 13 Batch 4750 Loss 0.8498 Accuracy 0.5167\n",
      "Epoch 13 Batch 4800 Loss 0.8510 Accuracy 0.5166\n",
      "Epoch 13 Batch 4850 Loss 0.8522 Accuracy 0.5164\n",
      "Epoch 13 Batch 4900 Loss 0.8535 Accuracy 0.5162\n",
      "Epoch 13 Batch 4950 Loss 0.8550 Accuracy 0.5160\n",
      "Epoch 13 Batch 5000 Loss 0.8563 Accuracy 0.5158\n",
      "Epoch 13 Batch 5050 Loss 0.8578 Accuracy 0.5156\n",
      "Epoch 13 Batch 5100 Loss 0.8592 Accuracy 0.5154\n",
      "Epoch 13 Batch 5150 Loss 0.8606 Accuracy 0.5151\n",
      "Epoch 13 Batch 5200 Loss 0.8617 Accuracy 0.5149\n",
      "Epoch 13 Batch 5250 Loss 0.8630 Accuracy 0.5146\n",
      "Epoch 13 Batch 5300 Loss 0.8642 Accuracy 0.5143\n",
      "Epoch 13 Batch 5350 Loss 0.8654 Accuracy 0.5139\n",
      "Epoch 13 Batch 5400 Loss 0.8666 Accuracy 0.5137\n",
      "Epoch 13 Batch 5450 Loss 0.8676 Accuracy 0.5134\n",
      "Epoch 13 Batch 5500 Loss 0.8687 Accuracy 0.5131\n",
      "Epoch 13 Batch 5550 Loss 0.8699 Accuracy 0.5129\n",
      "Epoch 13 Batch 5600 Loss 0.8709 Accuracy 0.5126\n",
      "Epoch 13 Batch 5650 Loss 0.8716 Accuracy 0.5123\n",
      "Epoch 13 Batch 5700 Loss 0.8724 Accuracy 0.5121\n",
      "Saving checkpoint for epoch 13 at ./TF/ckpt/ckpt-14\n",
      "Time taken for 1 epoch: 1467.1117758750916 secs\n",
      "\n",
      "Start of epoch 14\n",
      "Epoch 14 Batch 0 Loss 0.9964 Accuracy 0.5090\n",
      "Epoch 14 Batch 50 Loss 0.9718 Accuracy 0.4945\n",
      "Epoch 14 Batch 100 Loss 0.9687 Accuracy 0.4945\n",
      "Epoch 14 Batch 150 Loss 0.9728 Accuracy 0.4947\n",
      "Epoch 14 Batch 200 Loss 0.9726 Accuracy 0.4946\n",
      "Epoch 14 Batch 250 Loss 0.9725 Accuracy 0.4951\n",
      "Epoch 14 Batch 300 Loss 0.9686 Accuracy 0.4950\n",
      "Epoch 14 Batch 350 Loss 0.9672 Accuracy 0.4955\n",
      "Epoch 14 Batch 400 Loss 0.9626 Accuracy 0.4958\n",
      "Epoch 14 Batch 450 Loss 0.9601 Accuracy 0.4957\n",
      "Epoch 14 Batch 500 Loss 0.9582 Accuracy 0.4956\n",
      "Epoch 14 Batch 550 Loss 0.9594 Accuracy 0.4955\n",
      "Epoch 14 Batch 600 Loss 0.9595 Accuracy 0.4957\n",
      "Epoch 14 Batch 650 Loss 0.9579 Accuracy 0.4955\n",
      "Epoch 14 Batch 700 Loss 0.9564 Accuracy 0.4960\n",
      "Epoch 14 Batch 750 Loss 0.9565 Accuracy 0.4960\n",
      "Epoch 14 Batch 800 Loss 0.9561 Accuracy 0.4965\n",
      "Epoch 14 Batch 850 Loss 0.9544 Accuracy 0.4966\n",
      "Epoch 14 Batch 900 Loss 0.9539 Accuracy 0.4965\n",
      "Epoch 14 Batch 950 Loss 0.9530 Accuracy 0.4964\n",
      "Epoch 14 Batch 1000 Loss 0.9511 Accuracy 0.4964\n",
      "Epoch 14 Batch 1050 Loss 0.9502 Accuracy 0.4968\n",
      "Epoch 14 Batch 1100 Loss 0.9501 Accuracy 0.4969\n",
      "Epoch 14 Batch 1150 Loss 0.9481 Accuracy 0.4972\n",
      "Epoch 14 Batch 1200 Loss 0.9465 Accuracy 0.4974\n",
      "Epoch 14 Batch 1250 Loss 0.9446 Accuracy 0.4977\n",
      "Epoch 14 Batch 1300 Loss 0.9425 Accuracy 0.4981\n",
      "Epoch 14 Batch 1350 Loss 0.9416 Accuracy 0.4985\n",
      "Epoch 14 Batch 1400 Loss 0.9387 Accuracy 0.4992\n",
      "Epoch 14 Batch 1450 Loss 0.9359 Accuracy 0.4999\n",
      "Epoch 14 Batch 1500 Loss 0.9334 Accuracy 0.5007\n",
      "Epoch 14 Batch 1550 Loss 0.9312 Accuracy 0.5015\n",
      "Epoch 14 Batch 1600 Loss 0.9285 Accuracy 0.5023\n",
      "Epoch 14 Batch 1650 Loss 0.9262 Accuracy 0.5031\n",
      "Epoch 14 Batch 1700 Loss 0.9237 Accuracy 0.5039\n",
      "Epoch 14 Batch 1750 Loss 0.9213 Accuracy 0.5048\n",
      "Epoch 14 Batch 1800 Loss 0.9190 Accuracy 0.5058\n",
      "Epoch 14 Batch 1850 Loss 0.9165 Accuracy 0.5066\n",
      "Epoch 14 Batch 1900 Loss 0.9148 Accuracy 0.5072\n",
      "Epoch 14 Batch 1950 Loss 0.9129 Accuracy 0.5080\n",
      "Epoch 14 Batch 2000 Loss 0.9110 Accuracy 0.5087\n",
      "Epoch 14 Batch 2050 Loss 0.9083 Accuracy 0.5091\n",
      "Epoch 14 Batch 2100 Loss 0.9059 Accuracy 0.5094\n",
      "Epoch 14 Batch 2150 Loss 0.9037 Accuracy 0.5096\n",
      "Epoch 14 Batch 2200 Loss 0.9006 Accuracy 0.5098\n",
      "Epoch 14 Batch 2250 Loss 0.8982 Accuracy 0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Batch 2300 Loss 0.8957 Accuracy 0.5101\n",
      "Epoch 14 Batch 2350 Loss 0.8929 Accuracy 0.5104\n",
      "Epoch 14 Batch 2400 Loss 0.8903 Accuracy 0.5107\n",
      "Epoch 14 Batch 2450 Loss 0.8873 Accuracy 0.5110\n",
      "Epoch 14 Batch 2500 Loss 0.8848 Accuracy 0.5113\n",
      "Epoch 14 Batch 2550 Loss 0.8818 Accuracy 0.5117\n",
      "Epoch 14 Batch 2600 Loss 0.8794 Accuracy 0.5120\n",
      "Epoch 14 Batch 2650 Loss 0.8769 Accuracy 0.5122\n",
      "Epoch 14 Batch 2700 Loss 0.8748 Accuracy 0.5126\n",
      "Epoch 14 Batch 2750 Loss 0.8726 Accuracy 0.5129\n",
      "Epoch 14 Batch 2800 Loss 0.8705 Accuracy 0.5131\n",
      "Epoch 14 Batch 2850 Loss 0.8683 Accuracy 0.5133\n",
      "Epoch 14 Batch 2900 Loss 0.8664 Accuracy 0.5136\n",
      "Epoch 14 Batch 2950 Loss 0.8644 Accuracy 0.5139\n",
      "Epoch 14 Batch 3000 Loss 0.8630 Accuracy 0.5140\n",
      "Epoch 14 Batch 3050 Loss 0.8613 Accuracy 0.5143\n",
      "Epoch 14 Batch 3100 Loss 0.8598 Accuracy 0.5146\n",
      "Epoch 14 Batch 3150 Loss 0.8578 Accuracy 0.5148\n",
      "Epoch 14 Batch 3200 Loss 0.8562 Accuracy 0.5149\n",
      "Epoch 14 Batch 3250 Loss 0.8545 Accuracy 0.5151\n",
      "Epoch 14 Batch 3300 Loss 0.8526 Accuracy 0.5153\n",
      "Epoch 14 Batch 3350 Loss 0.8504 Accuracy 0.5156\n",
      "Epoch 14 Batch 3400 Loss 0.8484 Accuracy 0.5158\n",
      "Epoch 14 Batch 3450 Loss 0.8469 Accuracy 0.5161\n",
      "Epoch 14 Batch 3500 Loss 0.8455 Accuracy 0.5163\n",
      "Epoch 14 Batch 3550 Loss 0.8442 Accuracy 0.5165\n",
      "Epoch 14 Batch 3600 Loss 0.8422 Accuracy 0.5168\n",
      "Epoch 14 Batch 3650 Loss 0.8404 Accuracy 0.5171\n",
      "Epoch 14 Batch 3700 Loss 0.8390 Accuracy 0.5174\n",
      "Epoch 14 Batch 3750 Loss 0.8377 Accuracy 0.5177\n",
      "Epoch 14 Batch 3800 Loss 0.8365 Accuracy 0.5181\n",
      "Epoch 14 Batch 3850 Loss 0.8350 Accuracy 0.5185\n",
      "Epoch 14 Batch 3900 Loss 0.8336 Accuracy 0.5188\n",
      "Epoch 14 Batch 3950 Loss 0.8324 Accuracy 0.5191\n",
      "Epoch 14 Batch 4000 Loss 0.8309 Accuracy 0.5194\n",
      "Epoch 14 Batch 4050 Loss 0.8297 Accuracy 0.5197\n",
      "Epoch 14 Batch 4100 Loss 0.8288 Accuracy 0.5199\n",
      "Epoch 14 Batch 4150 Loss 0.8285 Accuracy 0.5200\n",
      "Epoch 14 Batch 4200 Loss 0.8287 Accuracy 0.5199\n",
      "Epoch 14 Batch 4250 Loss 0.8293 Accuracy 0.5199\n",
      "Epoch 14 Batch 4300 Loss 0.8302 Accuracy 0.5198\n",
      "Epoch 14 Batch 4350 Loss 0.8313 Accuracy 0.5197\n",
      "Epoch 14 Batch 4400 Loss 0.8323 Accuracy 0.5195\n",
      "Epoch 14 Batch 4450 Loss 0.8337 Accuracy 0.5193\n",
      "Epoch 14 Batch 4500 Loss 0.8351 Accuracy 0.5191\n",
      "Epoch 14 Batch 4550 Loss 0.8363 Accuracy 0.5190\n",
      "Epoch 14 Batch 4600 Loss 0.8378 Accuracy 0.5188\n",
      "Epoch 14 Batch 4650 Loss 0.8392 Accuracy 0.5186\n",
      "Epoch 14 Batch 4700 Loss 0.8407 Accuracy 0.5185\n",
      "Epoch 14 Batch 4750 Loss 0.8419 Accuracy 0.5183\n",
      "Epoch 14 Batch 4800 Loss 0.8431 Accuracy 0.5181\n",
      "Epoch 14 Batch 4850 Loss 0.8443 Accuracy 0.5179\n",
      "Epoch 14 Batch 4900 Loss 0.8454 Accuracy 0.5177\n",
      "Epoch 14 Batch 4950 Loss 0.8469 Accuracy 0.5175\n",
      "Epoch 14 Batch 5000 Loss 0.8483 Accuracy 0.5174\n",
      "Epoch 14 Batch 5050 Loss 0.8495 Accuracy 0.5171\n",
      "Epoch 14 Batch 5100 Loss 0.8509 Accuracy 0.5168\n",
      "Epoch 14 Batch 5150 Loss 0.8521 Accuracy 0.5166\n",
      "Epoch 14 Batch 5200 Loss 0.8535 Accuracy 0.5163\n",
      "Epoch 14 Batch 5250 Loss 0.8546 Accuracy 0.5160\n",
      "Epoch 14 Batch 5300 Loss 0.8557 Accuracy 0.5158\n",
      "Epoch 14 Batch 5350 Loss 0.8569 Accuracy 0.5155\n",
      "Epoch 14 Batch 5400 Loss 0.8581 Accuracy 0.5152\n",
      "Epoch 14 Batch 5450 Loss 0.8593 Accuracy 0.5150\n",
      "Epoch 14 Batch 5500 Loss 0.8603 Accuracy 0.5147\n",
      "Epoch 14 Batch 5550 Loss 0.8614 Accuracy 0.5144\n",
      "Epoch 14 Batch 5600 Loss 0.8625 Accuracy 0.5142\n",
      "Epoch 14 Batch 5650 Loss 0.8634 Accuracy 0.5139\n",
      "Epoch 14 Batch 5700 Loss 0.8644 Accuracy 0.5136\n",
      "Saving checkpoint for epoch 14 at ./TF/ckpt/ckpt-15\n",
      "Time taken for 1 epoch: 1487.3991088867188 secs\n",
      "\n",
      "Start of epoch 15\n",
      "Epoch 15 Batch 0 Loss 0.9511 Accuracy 0.4951\n",
      "Epoch 15 Batch 50 Loss 0.9647 Accuracy 0.4950\n",
      "Epoch 15 Batch 100 Loss 0.9684 Accuracy 0.4954\n",
      "Epoch 15 Batch 150 Loss 0.9628 Accuracy 0.4968\n",
      "Epoch 15 Batch 200 Loss 0.9631 Accuracy 0.4967\n",
      "Epoch 15 Batch 250 Loss 0.9592 Accuracy 0.4967\n",
      "Epoch 15 Batch 300 Loss 0.9577 Accuracy 0.4965\n",
      "Epoch 15 Batch 350 Loss 0.9534 Accuracy 0.4961\n",
      "Epoch 15 Batch 400 Loss 0.9557 Accuracy 0.4964\n",
      "Epoch 15 Batch 450 Loss 0.9539 Accuracy 0.4960\n",
      "Epoch 15 Batch 500 Loss 0.9514 Accuracy 0.4957\n",
      "Epoch 15 Batch 550 Loss 0.9521 Accuracy 0.4968\n",
      "Epoch 15 Batch 600 Loss 0.9520 Accuracy 0.4967\n",
      "Epoch 15 Batch 650 Loss 0.9520 Accuracy 0.4964\n",
      "Epoch 15 Batch 700 Loss 0.9499 Accuracy 0.4971\n",
      "Epoch 15 Batch 750 Loss 0.9497 Accuracy 0.4974\n",
      "Epoch 15 Batch 800 Loss 0.9495 Accuracy 0.4974\n",
      "Epoch 15 Batch 850 Loss 0.9493 Accuracy 0.4977\n",
      "Epoch 15 Batch 900 Loss 0.9485 Accuracy 0.4981\n",
      "Epoch 15 Batch 950 Loss 0.9473 Accuracy 0.4983\n",
      "Epoch 15 Batch 1000 Loss 0.9449 Accuracy 0.4980\n",
      "Epoch 15 Batch 1050 Loss 0.9443 Accuracy 0.4983\n",
      "Epoch 15 Batch 1100 Loss 0.9431 Accuracy 0.4985\n",
      "Epoch 15 Batch 1150 Loss 0.9419 Accuracy 0.4988\n",
      "Epoch 15 Batch 1200 Loss 0.9403 Accuracy 0.4989\n",
      "Epoch 15 Batch 1250 Loss 0.9385 Accuracy 0.4992\n",
      "Epoch 15 Batch 1300 Loss 0.9364 Accuracy 0.4996\n",
      "Epoch 15 Batch 1350 Loss 0.9343 Accuracy 0.4999\n",
      "Epoch 15 Batch 1400 Loss 0.9323 Accuracy 0.5006\n",
      "Epoch 15 Batch 1450 Loss 0.9294 Accuracy 0.5013\n",
      "Epoch 15 Batch 1500 Loss 0.9269 Accuracy 0.5022\n",
      "Epoch 15 Batch 1550 Loss 0.9241 Accuracy 0.5029\n",
      "Epoch 15 Batch 1600 Loss 0.9212 Accuracy 0.5039\n",
      "Epoch 15 Batch 1650 Loss 0.9184 Accuracy 0.5048\n",
      "Epoch 15 Batch 1700 Loss 0.9163 Accuracy 0.5056\n",
      "Epoch 15 Batch 1750 Loss 0.9141 Accuracy 0.5064\n",
      "Epoch 15 Batch 1800 Loss 0.9116 Accuracy 0.5072\n",
      "Epoch 15 Batch 1850 Loss 0.9095 Accuracy 0.5081\n",
      "Epoch 15 Batch 1900 Loss 0.9073 Accuracy 0.5089\n",
      "Epoch 15 Batch 1950 Loss 0.9056 Accuracy 0.5096\n",
      "Epoch 15 Batch 2000 Loss 0.9034 Accuracy 0.5102\n",
      "Epoch 15 Batch 2050 Loss 0.9008 Accuracy 0.5106\n",
      "Epoch 15 Batch 2100 Loss 0.8982 Accuracy 0.5110\n",
      "Epoch 15 Batch 2150 Loss 0.8958 Accuracy 0.5113\n",
      "Epoch 15 Batch 2200 Loss 0.8929 Accuracy 0.5115\n",
      "Epoch 15 Batch 2250 Loss 0.8903 Accuracy 0.5117\n",
      "Epoch 15 Batch 2300 Loss 0.8877 Accuracy 0.5118\n",
      "Epoch 15 Batch 2350 Loss 0.8852 Accuracy 0.5121\n",
      "Epoch 15 Batch 2400 Loss 0.8825 Accuracy 0.5123\n",
      "Epoch 15 Batch 2450 Loss 0.8799 Accuracy 0.5125\n",
      "Epoch 15 Batch 2500 Loss 0.8771 Accuracy 0.5128\n",
      "Epoch 15 Batch 2550 Loss 0.8744 Accuracy 0.5131\n",
      "Epoch 15 Batch 2600 Loss 0.8719 Accuracy 0.5134\n",
      "Epoch 15 Batch 2650 Loss 0.8695 Accuracy 0.5138\n",
      "Epoch 15 Batch 2700 Loss 0.8672 Accuracy 0.5141\n",
      "Epoch 15 Batch 2750 Loss 0.8651 Accuracy 0.5143\n",
      "Epoch 15 Batch 2800 Loss 0.8630 Accuracy 0.5146\n",
      "Epoch 15 Batch 2850 Loss 0.8607 Accuracy 0.5148\n",
      "Epoch 15 Batch 2900 Loss 0.8592 Accuracy 0.5151\n",
      "Epoch 15 Batch 2950 Loss 0.8572 Accuracy 0.5153\n",
      "Epoch 15 Batch 3000 Loss 0.8555 Accuracy 0.5156\n",
      "Epoch 15 Batch 3050 Loss 0.8538 Accuracy 0.5158\n",
      "Epoch 15 Batch 3100 Loss 0.8519 Accuracy 0.5160\n",
      "Epoch 15 Batch 3150 Loss 0.8503 Accuracy 0.5163\n",
      "Epoch 15 Batch 3200 Loss 0.8484 Accuracy 0.5166\n",
      "Epoch 15 Batch 3250 Loss 0.8464 Accuracy 0.5167\n",
      "Epoch 15 Batch 3300 Loss 0.8448 Accuracy 0.5169\n",
      "Epoch 15 Batch 3350 Loss 0.8428 Accuracy 0.5172\n",
      "Epoch 15 Batch 3400 Loss 0.8408 Accuracy 0.5175\n",
      "Epoch 15 Batch 3450 Loss 0.8392 Accuracy 0.5177\n",
      "Epoch 15 Batch 3500 Loss 0.8378 Accuracy 0.5180\n",
      "Epoch 15 Batch 3550 Loss 0.8362 Accuracy 0.5183\n",
      "Epoch 15 Batch 3600 Loss 0.8346 Accuracy 0.5186\n",
      "Epoch 15 Batch 3650 Loss 0.8329 Accuracy 0.5188\n",
      "Epoch 15 Batch 3700 Loss 0.8312 Accuracy 0.5192\n",
      "Epoch 15 Batch 3750 Loss 0.8297 Accuracy 0.5195\n",
      "Epoch 15 Batch 3800 Loss 0.8284 Accuracy 0.5198\n",
      "Epoch 15 Batch 3850 Loss 0.8272 Accuracy 0.5201\n",
      "Epoch 15 Batch 3900 Loss 0.8259 Accuracy 0.5204\n",
      "Epoch 15 Batch 3950 Loss 0.8245 Accuracy 0.5208\n",
      "Epoch 15 Batch 4000 Loss 0.8233 Accuracy 0.5210\n",
      "Epoch 15 Batch 4050 Loss 0.8220 Accuracy 0.5213\n",
      "Epoch 15 Batch 4100 Loss 0.8210 Accuracy 0.5215\n",
      "Epoch 15 Batch 4150 Loss 0.8210 Accuracy 0.5215\n",
      "Epoch 15 Batch 4200 Loss 0.8211 Accuracy 0.5216\n",
      "Epoch 15 Batch 4250 Loss 0.8214 Accuracy 0.5215\n",
      "Epoch 15 Batch 4300 Loss 0.8223 Accuracy 0.5214\n",
      "Epoch 15 Batch 4350 Loss 0.8232 Accuracy 0.5213\n",
      "Epoch 15 Batch 4400 Loss 0.8245 Accuracy 0.5211\n",
      "Epoch 15 Batch 4450 Loss 0.8260 Accuracy 0.5210\n",
      "Epoch 15 Batch 4500 Loss 0.8272 Accuracy 0.5209\n",
      "Epoch 15 Batch 4550 Loss 0.8284 Accuracy 0.5206\n",
      "Epoch 15 Batch 4600 Loss 0.8298 Accuracy 0.5204\n",
      "Epoch 15 Batch 4650 Loss 0.8309 Accuracy 0.5202\n",
      "Epoch 15 Batch 4700 Loss 0.8325 Accuracy 0.5200\n",
      "Epoch 15 Batch 4750 Loss 0.8338 Accuracy 0.5198\n",
      "Epoch 15 Batch 4800 Loss 0.8350 Accuracy 0.5196\n",
      "Epoch 15 Batch 4850 Loss 0.8363 Accuracy 0.5193\n",
      "Epoch 15 Batch 4900 Loss 0.8377 Accuracy 0.5191\n",
      "Epoch 15 Batch 4950 Loss 0.8392 Accuracy 0.5189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 Batch 5000 Loss 0.8406 Accuracy 0.5187\n",
      "Epoch 15 Batch 5050 Loss 0.8417 Accuracy 0.5184\n",
      "Epoch 15 Batch 5100 Loss 0.8429 Accuracy 0.5181\n",
      "Epoch 15 Batch 5150 Loss 0.8445 Accuracy 0.5179\n",
      "Epoch 15 Batch 5200 Loss 0.8459 Accuracy 0.5176\n",
      "Epoch 15 Batch 5250 Loss 0.8474 Accuracy 0.5173\n",
      "Epoch 15 Batch 5300 Loss 0.8484 Accuracy 0.5170\n",
      "Epoch 15 Batch 5350 Loss 0.8495 Accuracy 0.5167\n",
      "Epoch 15 Batch 5400 Loss 0.8507 Accuracy 0.5163\n",
      "Epoch 15 Batch 5450 Loss 0.8518 Accuracy 0.5161\n",
      "Epoch 15 Batch 5500 Loss 0.8528 Accuracy 0.5159\n",
      "Epoch 15 Batch 5550 Loss 0.8537 Accuracy 0.5156\n",
      "Epoch 15 Batch 5600 Loss 0.8546 Accuracy 0.5154\n",
      "Epoch 15 Batch 5650 Loss 0.8554 Accuracy 0.5152\n",
      "Epoch 15 Batch 5700 Loss 0.8564 Accuracy 0.5149\n",
      "Saving checkpoint for epoch 15 at ./TF/ckpt/ckpt-16\n",
      "Time taken for 1 epoch: 1526.1194899082184 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 15\n",
    "for epoch in range(EPOCHS):\n",
    "    print(\"Start of epoch {}\".format(epoch+1))\n",
    "    start = time.time()\n",
    "    \n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    \n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "        \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        \n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "        \n",
    "        if batch % 50 == 0:\n",
    "            print(\"Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}\".format(\n",
    "                epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print(\"Saving checkpoint for epoch {} at {}\".format(epoch+1,\n",
    "                                                        ckpt_save_path))\n",
    "    print(\"Time taken for 1 epoch: {} secs\\n\".format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "# Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "    inp_sentence=\\\n",
    "        [VOCAB_SIZE_EN-2]+tokenizer_en.encode(inp_sentence)+ [VOCAB_SIZE_EN-1]\n",
    "    enc_input=tf.expand_dims(inp_sentence,axis=0)\n",
    "    output=tf.expand_dims([VOCAB_SIZE_FR-2],axis=0)\n",
    "    \n",
    "    for _ in range(MAX_LENGTH):\n",
    "        predictions=transformer(enc_input,output,False)\n",
    "        prediction=predictions[:,-1:,:]\n",
    "        predicted_id=tf.cast(tf.argmax(prediction,axis=-1),tf.int32)\n",
    "        if predicted_id==VOCAB_SIZE_FR-1:\n",
    "            return tf.squeeze(output,axis=0)\n",
    "        output=tf.concat([output,predicted_id],axis=-1)\n",
    "    return (tf.squeeze(output,axis=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "    output=evaluate(sentence).numpy()\n",
    "    \n",
    "    predicted_sentence=tokenizer_fr.decode([i for i in output if i <VOCAB_SIZE_FR-2])\n",
    "    \n",
    "    print(\"Input:{}\".format(sentence))\n",
    "    print(\"Predicted_translation:{}\".format(predicted_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:This is my car\n",
      "Predicted_translation:Voil ma voiture.\n"
     ]
    }
   ],
   "source": [
    "translate(\"This is my car\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Transformer_for_NLP_udemy.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
